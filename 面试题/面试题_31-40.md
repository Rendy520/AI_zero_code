
# AI 零代码应用生成项目中，如何实现项目代码的打包下载功能？如何过滤掉不必要文件和目录的？

项目代码打包的核心是高效和纯净。我们使用了 Hutool 工具库的 `ZipUtil.zip()` 方法，它支持将文件目录直接流式压缩到 `HttpServletResponse` 的输出流中。

这种方式的好处是，它直接在内存中处理数据流，不仅节省了磁盘空间，也减少了 I/O 开销，下载响应更迅速。

为了确保用户下载到的是干净、可用的源代码，我们实现了一个黑名单过滤机制，在打包时排除掉所有不必要的文件和目录。过滤逻辑主要分为两类：

1.  按名称过滤：定义了一个忽略名称的集合，包含了如 `node_modules`、`dist` 构建产物、`.git`、`.idea`和 `.env`等目录或文件。
2.  按扩展名过滤：定义了一个忽略扩展名的集合，用于排除 `.log`、`.tmp`、`.cache`等。

在执行 `ZipUtil.zip()` 时，我传入了一个自定义的 `FileFilter`。这个过滤器会对项目目录下的每个文件和子目录进行递归检查，只有未命中上述任何一条黑名单规则的文件才会被包含在最终的压缩包内，保证交付给用户的代码包是纯净且安全的。


---


# AI 零代码应用生成项目中，你采用了 Spring Boot 接口预览、Nginx 部署的混合方案，这两种方式分别解决了什么问题？

这个混合方案是我们为满足不同阶段的需求而设计的，它结合了便捷性与生产级的性能和可靠性。

1）Spring Boot 接口预览解决实时反馈和开发便捷问题。

当 AI 生成或修改代码后，用户需要一个立即看到变更效果的途径。如果每次预览都依赖于完整的部署流程，效率会非常低下。

我们在后端通过一个`StaticResourceController`，直接将服务器上临时生成目录下的文件以静态资源的形式暴露出来。前端的预览 `iframe` 直接请求这个由 Spring Boot 应用提供的动态 URL。这种方式无需外部 Web 服务器介入，实现了生成即可见的实时预览，简化了开发调试流程。

2）Nginx 部署：解决生产性能和可靠性问题。

对于正式对外提供服务的应用，Nginx 作为专业的 Web 服务器，在处理高并发静态文件请求时性能远超 Spring Boot 内嵌的 Tomcat，并提供更强的稳定性和丰富的企业级功能，比如限流。

综上，Spring Boot 接口用于开发和预览阶段的快速迭代，而 Nginx 则用于正式部署阶段，确保最终应用的性能和可靠性。


---


# AI 零代码应用生成项目中，Nginx 承担了哪些职责？具体如何配置？

1.解决浏览器跨域的问题，并且利用反向代理的特性，将固定前缀的请求，发送到后端服务器。 2.对于普通应用，通过try\_files的配置，将所有未匹配到文件的请求，全部重定向到index.html入口页面 3.优化sse长连接的超时事件，保证vue工程化项目的流式输出不中断。


---


# AI 零代码应用生成项目中，AI 调用性能存在瓶颈的根本原因是什么？

调用性能瓶颈的根本原因是 LangChain4j 的 `ChatModel`实例在 Spring 容器中配置为了单例。并且虽然流式模型返回的是看似异步的 `Flux` 对象，但其底层的网络客户端在处理数据流时是同步阻塞的。

这就导致了当多个用户的请求同时到达时，由于只有一个 `ChatModel` 实例，它们必须排队等待。第一个请求会锁定这个单例实例，直到它与 AI 大模型的整个通信过程完全结束。在此期间，所有其他请求都被阻塞，无法执行。最终结果是，系统处理 AI 请求的行为从并发退化为了串行，导致后续用户需要等待前面所有请求完成后才能得到响应，严重影响性能和用户体验。

完整调用链如图：

![](https://pic.code-nav.cn/mianshiya/question_picture/markdown/OnoveWmT_202508221401325_mianshiya.png)

为了验证这个分析，我写了个单元测试，发现即使是不同的 AI Service 实例，只要使用的是同一个 `ChatModel`，依然会出现阻塞现象。

解决方案就是将 `ChatModel` 的 Bean 作用域从单例修改为 `prototype`，保证每次 AI 调用都使用一个全新的、独立的模型实例，实现真正的并发处理。


---


# AI 零代码应用生成项目中，Redis 主要用在哪些场景？

1）分布式会话管理：通过集成 `spring-session-data-redis`，我将用户的登录 Session 存储在 Redis 中。解决了单机 Session 在服务重启后会丢失的问题，并支持未来将系统水平扩展到多个服务器实例，而用户的登录状态不受影响。

2）AI 对话记忆持久化：为了让 AI 能够进行多轮对话，我使用 LangChain4j 提供的 `RedisChatMemoryStore` 来持久化每个应用的对话历史。保证 AI 的上下文记忆不会因服务重启而丢失，并且在分布式环境下也能保持一致。

3）热点数据缓存：针对主页精选应用列表这类访问频繁但更新较少的数据，我利用 Spring Cache 注解和 Redis 进行了缓存。首次请求时从数据库加载并存入 Redis，后续请求直接从 Redis 高速读取，显著降低数据库压力，加快页面加载速度。我还通过配置 CacheManager 自定义了缓存的过期时间，合理利用缓存空间。

4）分布式限流：为了防止 AI 接口被滥用并控制成本，我使用 Redisson 提供的分布式限流器 `RRateLimiter`。它是基于 Redis 实现的令牌桶限流，可以精确地对用户级别 / 接口级别 / IP 级别的 API 调用频率进行限制，为核心的 AI 生成服务提供了可靠的流量保护。


---


# 什么是 SSE？为什么 AI 生成代码时选择 SSE ？

SSE是一种允许服务器向客户端进行单向、持续性数据推送的Web技术，它建立在单个持久的 HTTP 连接之上。

我选择 SSE 的核心原因，是为了从根本上优化用户体验，解决长时间等待的问题。

传统 RESTful 接口 遵循 `请求-响应` 模式，客户端发送请求后，必须等待服务器完成所有处理并生成完整的响应后才能一次性接收。而 AI 代码生成是一个耗时任务，可能需要几十秒甚至几分钟。在此期间，用户面对的是一个静止的加载界面，无法获取到进度，体验非常差。

而 SSE 支持流式响应，AI 大模型是逐个 Token 生成内容的，利用 SSE，后端可以在每生成一小段代码片段时，就立刻通过长连接将其推送给前端。这就实现了打字机效果，用户能实时看到代码在屏幕上出现，明确知道系统正在工作，缩短了用户的感知等待时间，提升了用户体验。

利用 LangChain4j 提供的 Flux 响应式对象，我能够很轻松地获取到 AI 大模型实时输出的内容，并通过 SSE 返回给前端。

![](https://pic.code-nav.cn/mianshiya/question_picture/markdown/1QCFoenW_202508221402743_mianshiya.png)


---


# 如何优化调用 AI 接口的 SSE 流式输出效果？

在实现 SSE 流式输出时，我做了两项关键优化，确保数据传输的可靠性和完整性。

1）解决数据丢失问题：我发现在标准的 SSE 流中，连续的空格或特定的换行符可能会在传输过程中丢失或被浏览器错误解析。为了解决这个问题，我们将每个数据块都封装在一个 JSON 对象中进行传输，比如 `{"d": "..."}`。前端在接收到数据后，再从这个 JSON 对象中解析出 `d` 字段的原始数据。这种方式确保了无论内容是什么，都能被完整、准确地传输，避免了因内容特殊性导致的数据损坏。

2）明确流结束事件：标准的 SSE 在连接关闭时会触发 `onclose` 事件，但这个事件无法区分是正常结束还是网络异常中断。为了让前端能够明确地知道 AI 已经完成了所有内容的生成，我们在所有数据块发送完毕后，会额外发送一个名为 `done` 的自定义事件。前端通过监听这个 `done` 事件，就可以确信数据流已正常结束，从而触发后续的代码保存和预览更新等操作，增强整个流程的健壮性。


---


# AI 零代码应用生成项目中，你如何实现工具调用信息的流式输出？

针对 Vue 工程这种复杂场景，我利用了 LangChain4j 框架的工具调用机制，并对其流式输出进行了深度定制，在前端实现实时反馈。

整个流程的核心是将 AI 在工具调用过程中的不同阶段，封装成统一格式的流式消息对象，再通过 SSE 推送给前端。

1）定义了一个包含 `type` 字段的 `StreamMessage` 基类和几个子类，用来区分是普通的 AI 文本响应，还是工具调用的请求，或是工具执行完毕的结果。

![img](https://pic.code-nav.cn/mianshiya/question_picture/markdown/stTZwyXp_1753103456340-3307d403-73b1-4114-981c-780cd52085e0_mianshiya.png)

2）由于 LangChain4j 的 `Flux` 响应式流默认不直接暴露工具调用的中间状态，我使用了功能更强大的 `TokenStream`。通过参考 GitHub Issues 上的源码，我通过覆盖类路径的方式为 `TokenStream` 添加了 `onPartialToolExecutionRequest`等关键事件的监听器。

![img](https://pic.code-nav.cn/mianshiya/question_picture/markdown/1dufTX6O_1755783100163-ab6e1fc7-0844-46ee-b639-52491897e7a6_mianshiya.png)

并且，我将 TokenStream 返回的实时数据统一封装为了 JSON 格式的消息（如 AI 思考、发起工具调用、工具执行结果），便于下游处理：

```bash
{type="ai_response", data="为你生成代码"}

{type="tool_request", index=0, id="call_0", name="writeFile", arguments="流式参数"}
{type="tool_request", index=0, id="call_0", name="writeFile", arguments="流式参数"}
{type="tool_request", index=0, id="call_0", name="writeFile", arguments="流式参数"}
{type="tool_executed", index=0, id="call_0", name="writeFile", arguments="完整参数"}

{type="tool_request", index=1, id="call_1", name="writeFile", arguments="流式参数"}
{type="tool_request", index=1, id="call_1", name="writeFile", arguments="流式参数"}
{type="tool_request", index=1, id="call_1", name="writeFile", arguments="流式参数"}
{type="tool_executed", index=1, id="call_1", name="writeFile", arguments="完整参数"}

{type="ai_response", data="生成代码结束"}
```

3）我编写了一个 `JsonMessageStreamHandler` 处理器。它负责处理 `TokenStream` 封装好的 JSON 消息格式，并根据情况返回给前端或者保存对话记忆到数据库中。

为了简化逻辑，我还做了一个优化。对于同一个工具的多次流式调用请求，只在第一次出现时向前端推送一个 “选择工具” 的提示，后续具体的参数流不再实时透传，而是在工具执行完毕后，将完整的调用信息一次性格式化后推送。

整个 AI 流式处理过程如图：

![](https://pic.code-nav.cn/mianshiya/question_picture/markdown/x1tIULGA_202508221402275_mianshiya.png)

通过这个流程，前端就能根据收到的不同类型的消息，实时地向用户展示 AI 的完整工作流，比如 “【选择工具】写入文件”、“【调用工具】写入文件 `App.vue`”，实现了复杂工具调用过程的清晰、实时的流式输出。


---


# AI 零代码应用生成项目中，如何为 AI 对话接口实现限流？解释一下原理？

我为 AI 对话这个核心且高成本的接口设计了一套基于 Redisson 的分布式限流方案，防止恶意请求和资源滥用。

首先我创建了一个自定义的 `@RateLimit` 注解，可以方便地标记在需要限流的 Controller 方法上。这个注解允许开发者灵活配置限流规则，比如限流的 key、速率、时间窗口以及限流类型。

通过 Spring AOP，我编写了一个 `RateLimitAspect` 切面，它会拦截所有标记了 `@RateLimit` 注解的方法。在方法执行前，切面会执行核心的限流逻辑。切面会根据注解中定义的限流类型动态生成一个唯一的 Redis Key。比如，如果是用户级别限流，Key 会包含 `user:` 前缀和当前用户的 ID；如果是 IP 级别，则包含 `ip:` 前缀和客户端 IP 地址。

我的限流逻辑是基于 Redisson 提供的分布式限流器 `RRateLimiter` 实现的，底层采用了令牌桶算法。

令牌桶算法的原理如下：

1.  系统在 Redis 中为每个限流 Key 维护一个虚拟的、固定容量的令牌桶。
2.  系统会以一个恒定的速率向这个桶里放入令牌。
3.  每当一个请求到达时，它必须尝试从桶里获取一个令牌。
4.  如果桶里有足够的令牌，请求成功获取并被处理；如果桶里没有令牌了，说明请求过于频繁，该请求就会被拒绝，并抛出一个自定义的业务异常，提示用户请求过于频繁。

![](https://pic.code-nav.cn/mianshiya/question_picture/markdown/2KUFhcFY_202508221403812_mianshiya.png)

可以看这道面试题 [了解常见的限流算法和实现原理](https://www.mianshiya.com/question/1802901573531410433)。


---


# 什么是可观测性？为什么需要为 AI 项目构建一套可观测性体系？

1.可观测性，是将系统中的性能指标和负载情况，以图标的形式在前端页面上进行展示。 2.观测分为系统性能监控和业务监控，系统性能监控的目的是，观察高并发情况下系统的负载，垃圾回收和jvm使用情况，以进行性能优化。业务监控则是对用户使用系统的习惯进行分析，以改进具体的功能。
