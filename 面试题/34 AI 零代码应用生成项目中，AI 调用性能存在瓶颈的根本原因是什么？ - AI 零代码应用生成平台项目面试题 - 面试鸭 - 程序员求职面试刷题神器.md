
# AI 零代码应用生成项目中，AI 调用性能存在瓶颈的根本原因是什么？

调用性能瓶颈的根本原因是 LangChain4j 的 `ChatModel`实例在 Spring 容器中配置为了单例。并且虽然流式模型返回的是看似异步的 `Flux` 对象，但其底层的网络客户端在处理数据流时是同步阻塞的。

这就导致了当多个用户的请求同时到达时，由于只有一个 `ChatModel` 实例，它们必须排队等待。第一个请求会锁定这个单例实例，直到它与 AI 大模型的整个通信过程完全结束。在此期间，所有其他请求都被阻塞，无法执行。最终结果是，系统处理 AI 请求的行为从并发退化为了串行，导致后续用户需要等待前面所有请求完成后才能得到响应，严重影响性能和用户体验。

完整调用链如图：

![](https://pic.code-nav.cn/mianshiya/question_picture/markdown/OnoveWmT_202508221401325_mianshiya.png)

为了验证这个分析，我写了个单元测试，发现即使是不同的 AI Service 实例，只要使用的是同一个 `ChatModel`，依然会出现阻塞现象。

解决方案就是将 `ChatModel` 的 Bean 作用域从单例修改为 `prototype`，保证每次 AI 调用都使用一个全新的、独立的模型实例，实现真正的并发处理。
