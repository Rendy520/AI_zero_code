
# 为什么 AI 零代码应用生成项目中要配置多种不同的 AI 大模型？

项目中配置多种不同能力和成本的 AI 模型，主要是出于成本优化和性能优化的综合考量。

核心思想是为不同的任务匹配最合适的模型，避免杀鸡用牛刀。

一个复杂的 AI 应用平台通常包含多种不同类型的 AI 任务，它们的复杂度和对模型能力的要求也各不相同。

-   复杂推理任务：比如生成完整的 Vue 工程项目，这需要大模型具备强大的代码理解、逻辑推理和遵循复杂指令的能力。对于这类任务，我配置了性能更强但成本也更高的模型，如 `deepseek-reasoner`，并为其设置了较大的 `max-tokens` 和较低的 `temperature` 保证生成质量和稳定性。
-   简单分类任务：比如 AI 智能路由，本质是一个简单的分类任务，只需要根据用户提示词判断应该选择哪种代码生成方案。这类任务对模型的推理能力要求不高。因此，我配置了更轻量、更快速且成本极低的模型，如阿里云百炼的 `qwen-turbo`。`qwen-turbo` 的百万 tokens 输出成本比 `deepseek-chat` 便宜十倍以上，显著降低智能路由功能的成本。

为了实现这种策略，我在 `application.yml` 文件中为不同场景定义了独立的模型配置块，比如 `reasoning-streaming-chat-model` 和 `routing-chat-model`。然后，通过独立的 `@Configuration` 配置类来创建这些模型的 Bean 实例。最后，在相应的 AI Service 工厂中，根据业务场景注入并使用特定的模型实例。

这样在保证复杂任务生成质量的同时，大幅降低了简单任务的成本和响应延迟，实现了系统整体的降本增效。
