
# 描述一下 AI 大模型生成 Vue 工程项目的过程？

生成 Vue 工程项目比生成单个 HTML 文件要复杂得多，因为它涉及到多个文件和目录的创建。为此，我设计了一套基于 AI 工具调用的自动化流程。

分为以下几个关键步骤：

1）AI 规划与工具调用：当用户请求生成一个 Vue 工程时，系统会使用一个专门为 Vue 项目设计的、包含详细约束的系统提示词来调用大模型。这个提示词要求 AI 必须通过调用我提供的 `writeFile` 工具来逐个创建项目文件。AI 会首先在内部规划出需要创建的文件列表，如 `package.json`、`vite.config.js`、`src/main.js`、`src/App.vue` 等，然后按顺序、多次调用 `writeFile` 工具。

2）后端文件写入：我在后端实现了一个 `FileWriteTool` 工具类。当 AI 调用此工具时，LangChain4j 框架会捕获该请求并执行 `writeFile` 方法。该方法会根据 AI 提供的相对路径和文件内容，在一个以 `vue_project_{appId}` 命名的专属目录下创建或覆写相应的文件，保证每个应用的文件隔离。

3）流式过程反馈：为了让用户能实时看到生成进度，整个工具调用过程是流式返回给前端的。我设计了一套自定义的 JSON 消息格式，用来区分不同的事件类型，如 `AI_RESPONSE`、`TOOL_REQUEST`和 `TOOL_EXECUTED`。前端接收到这些消息后，会解析并展示给用户。

4）自动化构建：当 AI 完成所有文件的写入后，`JsonMessageStreamHandler` 会在流结束时异步触发 `VueProjectBuilder` 服务。该服务会自动在服务器上对应的项目目录中执行 `npm install` 和 `npm run build` 命令，安装依赖并打包构建项目。

5）预览与部署：构建成功后会生成一个 `dist` 目录。前端的预览 `iframe` 和最终的应用部署，都会指向这个 `dist` 目录，让用户能够看到最终可运行的应用效果。

整个过程如图：

![](https://pic.code-nav.cn/mianshiya/question_picture/markdown/9NWQ1VIM_202508221359572_mianshiya.png)


---


# 给 AI 的文件写入工具是怎么实现的？如何确保每个应用的文件都写入到隔离的目录下？

文件写入工具 `FileWriteTool` 是我自定义的一个 LangChain4j 工具， AI 在执行代码生成任务时，将代码内容写入到服务器的指定文件中。

这个工具的核心是一个 `writeFile` 方法，使用了 `@Tool` 注解进行标记，LangChain4j 框架能够识别和调用。方法的参数也使用了 `@P` 注解加以描述，让 AI 理解如何正确传参。

为了知道文件应该写入哪个应用的目录，`writeFile` 方法的参数列表中包含了一个特殊的参数 `@ToolMemoryId Long appId`。这个注解是 LangChain4j 提供的上下文传参能力，它能将调用 AI Service 时指定的 `memoryId` 自动注入到工具方法中。

在方法内部，为了确保文件隔离，它首先会根据传入的 `appId` 构建一个唯一的项目根目录路径，格式为 `vue_project_{appId}`。所有的文件写入操作都被严格限制在这个目录内。

它接收 AI 传入的相对文件路径，并将其与上面构建的项目根目录进行拼接，得到一个绝对路径。然后，它会检查并创建所有必要的父目录，最后使用 `Files.write` 将内容写入文件。

为了防止泄露服务器的绝对路径，工具执行成功后，返回给 AI 和用户的信息中只包含相对路径，比如 “文件写入成功：src/App.vue”。


---


# 什么是 AI 智能路由？你如何利用 AI 根据用户需求自动选择最合适的代码生成方案？

AI 智能路由是一种利用大模型进行决策，根据用户输入自动选择最合适处理流程的机制。在本项目中，我用它来解决一个核心问题：当用户提出需求时，系统应该自动判断是使用原生 HTML、原生多文件还是 Vue 工程这三种方案中的哪一种来生成应用。

![](https://pic.code-nav.cn/mianshiya/question_picture/markdown/KCKfc3MT_202508221359297_mianshiya.png)

我的实现方案主要基于 LangChain4j 的结构化输出能力：

1）我编写了一个专门的系统提示词 `codegen-routing-system-prompt.txt`。这个提示词赋予 AI “代码生成方案路由器” 的角色，并明确列出了可选的方案以及详细的判断规则。比如：

-   如果用户需求简单，只需要一个展示页面，选择 HTML
-   如果用户需求复杂，涉及多页面、复杂交互、数据管理等，选择 VUE\_PROJECT

2）我定义了一个 `AiCodeGenTypeRoutingService` 接口，其核心方法 `routeCodeGenType` 的返回值直接就是 `CodeGenTypeEnum` 这个枚举类型。通过 LangChain4j 的结构化输出机制，AI 的决策结果会被自动转换为对应的枚举值，无需手动解析字符串。

3）在用户创建应用的 `createApp` 业务方法中，我会调用这个智能路由服务。将用户的初始 `initPrompt` 作为输入，获取 AI 推荐的 `CodeGenTypeEnum`，然后将这个结果存入数据库的 `app` 表中。后续的代码生成流程就会根据这个类型来执行相应的逻辑。

4）此外，由于路由决策是一个相对简单的分类任务，我为这个功能配置了成本更低、响应更快的轻量级大模型，在保证决策质量的同时优化了系统的整体成本和性能。

通过这种方式，系统实现了智能化的流程选择，不仅提升用户体验，还为不同复杂度的任务匹配了最合适的处理资源。


---


# 什么是 LangChain4j 的护轨机制？你是如何利用输入护轨来防止恶意 Prompt 输入和注入攻击的？

LangChain4j 的护轨机制是一套用于保障 AI 应用安全和稳定性的拦截器系统，类似于 Web 应用中的过滤器或拦截器。它分为两种：

-   输入护轨 (InputGuardrail)：在用户的输入发送给大模型之前进行检查。
-   输出护轨 (OutputGuardrail)：在大模型返回响应之后，但在内容呈现给用户之前进行检查。

![img](https://pic.code-nav.cn/mianshiya/question_picture/markdown/bgQYoLYi_1755762526746-44c39aa1-4afa-49d4-abb9-128ff0bf7bf1_mianshiya.png)

在项目中，我主要利用输入护轨来防御恶意的 Prompt 输入和常见的注入攻击：

1）创建自定义护轨：我实现了一个 `PromptSafetyInputGuardrail` 类，它继承了 `InputGuardrail` 接口。

2）实现多维度校验：在这个类的 `validate` 方法中，我定义了多层校验逻辑来审查用户的输入内容：

-   长度限制：检查输入是否超过预设的最大长度，防止超长 Prompt 消耗过多资源。
-   敏感词过滤：维护一个静态的敏感词列表，包含如“忽略之前的指令”、“破解”、“越狱”等词汇。如果输入中包含这些词，请求将被拒绝。
-   注入模式检测：定义了一系列正则表达式，用于匹配常见的 Prompt 注入攻击模式，比如试图让 AI 忘记身份、扮演其他角色或遵循新指令的语句。如果匹配成功，同样会拦截请求。

3）拦截与响应：如果任何一项检查失败，`validate` 方法会返回 `fatal("错误信息")`，这会立即中断 AI 调用流程并抛出异常，阻止恶意输入被发送到大模型。

4）集成到 AI Service：最后，在 `AiCodeGeneratorServiceFactory` 中构建 AI Service 实例时，通过 `.inputGuardrails()` 方法将这个自定义的护轨注册进去。


---


# 在 AI 大模型响应不稳定时，你如何利用输出护轨实现自动重试？

由于大模型的输出具有不确定性，有时可能会返回空内容、过短的内容或不符合要求的内容。为了提升系统的稳定性和生成质量，我利用了 LangChain4j 的输出护轨机制实现了一套自动重试和修正的策略。

1）创建自定义输出护轨：我实现了一个 `RetryOutputGuardrail` 类，它继承了 `OutputGuardrail` 接口。这个护轨的核心作用是在大模型返回响应后，对其内容进行质量检查。

2）定义校验规则：在 `validate` 方法中，我编写了检查逻辑：

-   空内容检查：判断响应内容是否为 null 或空字符串。
-   长度检查：判断响应内容的长度是否小于一个合理的阈值（比如 10 个字符）。
-   敏感内容检查：检查响应中是否意外包含了密码、API Key 等敏感信息。

3）触发智能重试：如果检查发现响应内容不合格，`validate` 方法不会直接失败，而是返回一个 `reprompt`结果。比如，如果内容过短，它会返回 `reprompt("响应内容过短", "请提供更详细的内容")`。

LangChain4j 框架接收到这个结果后，会自动将 **修正指令** 追加到原始对话历史中，然后重新向大模型发起一次调用，引导 AI 生成更符合要求的回复。

4）允许正常通过：如果响应通过了所有检查，方法会返回 `success()`，允许内容正常传递给后续流程。

5）集成到 AI Service：最后，在 `AiCodeGeneratorServiceFactory` 中，通过 `.outputGuardrails()` 方法将 `RetryOutputGuardrail` 注册到 AI Service 中。


---


# 为什么 AI 零代码应用生成项目中要配置多种不同的 AI 大模型？

项目中配置多种不同能力和成本的 AI 模型，主要是出于成本优化和性能优化的综合考量。

核心思想是为不同的任务匹配最合适的模型，避免杀鸡用牛刀。

一个复杂的 AI 应用平台通常包含多种不同类型的 AI 任务，它们的复杂度和对模型能力的要求也各不相同。

-   复杂推理任务：比如生成完整的 Vue 工程项目，这需要大模型具备强大的代码理解、逻辑推理和遵循复杂指令的能力。对于这类任务，我配置了性能更强但成本也更高的模型，如 `deepseek-reasoner`，并为其设置了较大的 `max-tokens` 和较低的 `temperature` 保证生成质量和稳定性。
-   简单分类任务：比如 AI 智能路由，本质是一个简单的分类任务，只需要根据用户提示词判断应该选择哪种代码生成方案。这类任务对模型的推理能力要求不高。因此，我配置了更轻量、更快速且成本极低的模型，如阿里云百炼的 `qwen-turbo`。`qwen-turbo` 的百万 tokens 输出成本比 `deepseek-chat` 便宜十倍以上，显著降低智能路由功能的成本。

为了实现这种策略，我在 `application.yml` 文件中为不同场景定义了独立的模型配置块，比如 `reasoning-streaming-chat-model` 和 `routing-chat-model`。然后，通过独立的 `@Configuration` 配置类来创建这些模型的 Bean 实例。最后，在相应的 AI Service 工厂中，根据业务场景注入并使用特定的模型实例。

这样在保证复杂任务生成质量的同时，大幅降低了简单任务的成本和响应延迟，实现了系统整体的降本增效。


---


# 当 AI 大模型调用工具时，可能会陷入无限循环，你是如何避免这个问题的？

1.将对话记忆上下文的容量调大，避免超过限制，ai无法得到自己之前生成记录的上下文。 2.自定义一个终止工具，并且注册，这样ai会自主判断，当陷入循环，或者无法继续生成时，则会调用该工具进行终止操作。 3.在ai service工厂创建ai service代理时，设置调用工具的次数上限


---


# 为什么在查询对话历史时，你选择使用游标分页而不是传统的分页？

在查询对话历史这类实时性强、数据可能频繁变动的场景下，我选择使用游标分页，主要是为了解决传统 `limit offset` 分页带来的数据一致性和性能两大问题。

1）数据一致性问题：传统的 `limit offset` 分页是基于数据行的位置偏移来获取数据的。在一个活跃的对话中，如果用户正在加载第二页历史记录时，又有新消息插入到数据库的开头，会导致数据整体下移。这样，用户请求的第二页数据可能会包含第一页已经看过的消息，造成数据重复；反之，如果删除了消息，则可能导致用户错过某些记录。

游标分页通过使用一个稳定不变的参照点（即游标），比如上一页最后一条消息的创建时间 `createTime`，来获取下一页数据（`WHERE createTime < '游标值'`），避免了这个问题。

![img](https://pic.code-nav.cn/mianshiya/question_picture/markdown/xkMsjY1U_1753773321162-3a635a77-f108-4d16-b85e-ad06e06bef67_mianshiya.png)

2）性能问题：当对话历史数据量非常大时，`limit offset` 分页的性能会急剧下降。比如，查询 `LIMIT 10000, 10` 时，数据库需要先扫描并跳过前面的 10000 条记录，才能找到需要返回的10条数据，这个开销非常大。游标分页的查询 `WHERE createTime < '游标值' ORDER BY createTime DESC LIMIT 10`，可以利用索引直接定位到游标所在的位置，然后顺序读取 10 条记录，查询性能非常高且稳定，不会随着页数的增加而变慢。

综上，考虑到对话历史的业务特性，使用游标分页是更优的选择，它能提供更准确、更高效的数据加载体验。


---


# AI 零代码应用生成项目的对话历史表的索引是如何设计的？

`chat_history` 表的索引设计是为了高效地支持对话历史的查询，特别是游标分页。

```lua
-- 对话历史表
create table chat_history
(
    -- ... 其他字段
    INDEX idx_appId (appId),                       -- 提升基于应用的查询性能
    INDEX idx_createTime (createTime),             -- 提升基于时间的查询性能
    INDEX idx_appId_createTime (appId, createTime) -- 游标查询核心索引
) comment '对话历史' collate = utf8mb4_unicode_ci;
```

我主要设计了以下几个索引：

1）`idx_appId`：在 `appId` 字段上建立索引。这个索引主要用于快速筛选出特定应用的所有对话历史，比如在删除应用时需要关联删除其所有对话记录的场景。

2）`idx_createTime`：在 `createTime` 字段上建立索引。这个索引主要服务于管理员查询所有对话历史并按时间排序的需求。

3）`idx_appId_createTime`：这是最核心的一个索引，它同时包含了 `appId` 和 `createTime` 两个字段。这个复合索引是专门为游标分页查询优化的。当执行类似 `SELECT * FROM chat_history WHERE appId = ? AND createTime < ? ORDER BY createTime DESC LIMIT 10` 的查询时，数据库可以利用这个索引：

-   首先快速定位到特定 `appId` 的数据范围。
-   然后在 `appId` 相同的数据内部，继续利用索引快速找到 `createTime` 小于游标值的位置。
-   最后，由于索引本身是按 `createTime` 排序的，数据库可以直接从该位置顺序读取所需的10条记录，避免了全表扫描和文件排序，提升查询效率。


---


# 在实现 AI 的对话记忆时，为什么选择 Redis 进行持久化？

## 

12038\. 在实现 AI 的对话记忆时，为什么选择 Redis 进行持久化？ 

在项目中，我选择使用 Redis 进行对话记忆持久化，主要是考虑以下几点：

-   性能优势：Redis 作为一个内存数据库，其读写性能远高于传统的 MySQL 数据库。对于需要频繁读写的对话记忆场景，使用 Redis 可以提供更快的响应速度。
-   避免内存溢出：如果直接使用内存来存储会话记忆，当应用数量和对话轮次增多时，会占用大量 JVM 内存，容易导致内存溢出（OOM）问题。而 Redis 作为独立的持久化服务，可以有效避免这个问题。
-   数据一致性与重启恢复：与内存记忆相比，Redis 可以在服务重启后依然保留对话状态，保证了数据的持久性。
-   与业务数据隔离：虽然数据库中也有对话记忆，但是 `chat_history` 表存储的是完整的、用于长期追溯的对话记录，包含了其他业务字段。而 AI 的对话记忆只需要纯粹的对话上下文，将两者分开存储，可以避免直接将业务数据表暴露给 LangChain4j 的记忆组件管理，使架构更清晰。

作者：![](https://pic.code-nav.cn/user_avatar/1772087337535152129/thumbnail/8stgwMmT-50265-logo.3c8859f8.png)

分享最高赚 ¥64.5

![微信小程序](https://www.mianshiya.com/assets/svg/miniapp.svg)

[![Ray丶丶 的用户头像](https://thirdwx.qlogo.cn/mmopen/vi_32/DYAIOgq83eqhf9NQyvibXhNGyhziaOsh2cB0FTiaLuXaS0FWdzw2lnYq1bHIheOW8Oue3CCZ46f6S4WiaicazuTiaQLQ/132)](https://www.mianshiya.com/user/1821431596438986753)

-   [![hjx 的用户头像](https://pic.code-nav.cn/user_avatar/1951107463568228354/thumbnail/lrGd3Onum59E2Twh.png)](https://www.mianshiya.com/user/1951107463568228354)
    
    #### 
    
    ![等级图标](https://www.mianshiya.com/_next/image?url=%2Fassets%2Flevel%2Flevel3.png&w=96&q=75)
    
    我来回答一下这个问题。 在实现 AI 的对话记忆时，我选择使用 Redis 进行持久化，主要是考虑以下几个方面：
    
    1.  性能优势 Redis 作为一个内存数据库，其读写性能远高于传统的 MySQL 数据库。对于需要频繁读写的对话记忆场景，使用 Redis 可以提供更快的响应速度。比如，每次对话都需要
    
-   [![心在路上 的用户头像](https://thirdwx.qlogo.cn/mmopen/vi_32/UIrPVwK16j7txV04H47jVf1cEom01SWNtogra3v7NyF5SHtHkBXfvbTl7EneMNjO833lvoDvsibjyYUTBxt1gctFBhHlpIhKiaicHQ1cyPQXUs/132)](https://www.mianshiya.com/user/1817038876463783937)
    
    #### 
    
    ![等级图标](https://www.mianshiya.com/_next/image?url=%2Fassets%2Flevel%2Flevel1.png&w=96&q=75)
    
    在实现 AI 的对话记忆时，为什么选择 Redis 进行持久化?
    
    1.  redis 基于内存，读写非常快，性能比较好
    2.  避免内存溢出，langchain4j 默认实现是基于内存的，如果对话轮次比较多的情况下，会产生OOM
    3.  数据一致性和重启恢复：与内存记忆相比，redis 在服务重启之后也会
    
-   [![yyc 的用户头像](https://pic.code-nav.cn/user_avatar/1916159505222971394/thumbnail/PSM9Rzkngc9fvC59.jpg)](https://www.mianshiya.com/user/1916159505222971394)
    
    #### 
    
    ![等级图标](https://www.mianshiya.com/_next/image?url=%2Fassets%2Flevel%2Flevel2.png&w=96&q=75)
    
    在实现 AI 的对话记忆时，为什么选择 Redis 进行持久化?
    
    选择 Redis 进行对话记忆持久化基于以下考虑：
    
    1.  **性能优势**：Redis 作为内存数据库，读写性能远高于 MySQL，满足对话记忆频繁读写的响应速度需求。
        
    2.  **避免内存溢出**：直接使用内存存储会话记忆，应用
        
    
-   [![axing 的用户头像](https://pic.code-nav.cn/user_avatar/1962348711956758529/thumbnail/cERF6MkiAwhQKbNq.jpeg)](https://www.mianshiya.com/user/1962348711956758529)
    
    #### 
    
    ![等级图标](https://www.mianshiya.com/_next/image?url=%2Fassets%2Flevel%2Flevel3.png&w=96&q=75)
    
    在实现 **AI 对话记忆** 时，我选择用 **Redis 来做持久化**，主要考虑到以下几点：
    
    1.  **性能优势**：Redis 是内存数据库，读写速度远快于 MySQL，非常适合频繁访问的对话记忆场景。
    2.  \*\*避免
    
-   [![给我一个lemon 的用户头像](https://pic.code-nav.cn/user_avatar/1834514345856004098/thumbnail/UU1McX5z3gFPoTin.jpg)](https://www.mianshiya.com/user/1834514345856004098)
    
    #### 
    
    ![等级图标](https://www.mianshiya.com/_next/image?url=%2Fassets%2Flevel%2Flevel3.png&w=96&q=75)
    
    ## 在实现AI的对话记忆时，为什么选择Redis进行持久化？
    
    -   性能优势：Redis作为一个内存数据库，读写速度非常快。对于频繁需要读写的对话记忆场景，使用Redis可以提供更快的响应速度
    -   避免内存溢出：如果直接使用内存来存储会话记忆，当应用数量和对话轮次增多时，会占用大量JVM内存，容易导
