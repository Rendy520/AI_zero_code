
# 在 AI 大模型响应不稳定时，你如何利用输出护轨实现自动重试？

由于大模型的输出具有不确定性，有时可能会返回空内容、过短的内容或不符合要求的内容。为了提升系统的稳定性和生成质量，我利用了 LangChain4j 的输出护轨机制实现了一套自动重试和修正的策略。

1）创建自定义输出护轨：我实现了一个 `RetryOutputGuardrail` 类，它继承了 `OutputGuardrail` 接口。这个护轨的核心作用是在大模型返回响应后，对其内容进行质量检查。

2）定义校验规则：在 `validate` 方法中，我编写了检查逻辑：

-   空内容检查：判断响应内容是否为 null 或空字符串。
-   长度检查：判断响应内容的长度是否小于一个合理的阈值（比如 10 个字符）。
-   敏感内容检查：检查响应中是否意外包含了密码、API Key 等敏感信息。

3）触发智能重试：如果检查发现响应内容不合格，`validate` 方法不会直接失败，而是返回一个 `reprompt`结果。比如，如果内容过短，它会返回 `reprompt("响应内容过短", "请提供更详细的内容")`。

LangChain4j 框架接收到这个结果后，会自动将 **修正指令** 追加到原始对话历史中，然后重新向大模型发起一次调用，引导 AI 生成更符合要求的回复。

4）允许正常通过：如果响应通过了所有检查，方法会返回 `success()`，允许内容正常传递给后续流程。

5）集成到 AI Service：最后，在 `AiCodeGeneratorServiceFactory` 中，通过 `.outputGuardrails()` 方法将 `RetryOutputGuardrail` 注册到 AI Service 中。
