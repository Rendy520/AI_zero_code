---
source: https://www.codefather.cn/course/1948291549923344386/section/1955850416717950978
---

# 11 - 系统优化 - 【大厂必备】LangChain4j + 工作流

## 本节重点

任何项目都是先完成再完美，经过的学习，我们的 AI 零代码应用生成⁠平台已经具备了完整的功能。但是，作为一个有追⁠求的程序员，怎么能满足于 “能跑就行” 呢？这一节咱们就从多个维度对系统进行全面优化，让它从玩具项目真正蜕变为生产级别的应用。

本节将从以下几个方面进行系统优化：

-   性能优化：
    
    -   AI 并发调用：解决目前只能同时处理一个 AI 请求的瓶颈问题
    -   Redis 缓存：通过缓存主页精选内容提升响应速度
-   实时性优化：
    
    -   实时浏览：确保用户生成网站后能立刻看到最新效果
-   安全性优化：
    
    -   流量保护：为 AI 对话接口实现限流限频机制
    -   Prompt 审查：防范恶意输入和注入攻击
-   稳定性优化：
    
    -   重试策略：通过护轨机制提升系统容错能力
    -   工具调用优化：解决 AI 工具调用的无限循环问题
-   成本优化：
    
    -   AI 大模型成本控制：根据不同场景选择合适的模型

## 一、性能优化

### AI 并发调用问题

#### 问题分析

在实际使用过程中，我们发现了一个严重的性能瓶颈：当多⁠个用户同时使用平台时，只有第一个用⁠户的 AI 请求能够正常处理，后续的请求都会被阻塞，需要等待前面的请求完全处理完毕后才能开始执行。

用户量较少时可能不太明显，但随着平台用户的增长，这个问题会变得越发严⁠重。想象一下，如果有 10 个用户同时想要生成网站，⁠第 10 个用户可能需要等待几分钟以上才能看到 AI 开始响应，肯定不行                                

那么，这个问题的根源在哪里呢？

经过分析，发现问题出在 AI 大模型的 `ChatModel` 采用了单例模式。虽然 `StreamingChatModel` 返回的是 Flux 响应式流，表面上看起来是异步的，但其底层的 `SpringRestClient.execute()` 方法内部实际上是同步解析数据流的，导致了串行执行问题。

完整调用链如图：

![](https://pic.code-nav.cn/course_picture/1608440217629360130/Bfme5jUmFhwLzknW.webp)

为了验证这个分析，我写了个单元测试，发现即使是不同的 AI Service 实例，只要使用的是同一个 `ChatModel`，依然会出现阻塞现象。

#### 解决方案 - 多例模式

寻找解决方案的过程中，我发现 LangChain4j 官方提到，使用不同的对话记忆 id 就能解决并发问题（参考 [GitHub Issue #2755](https://github.com/langchain4j/langchain4j/issues/2755)）。但其实这只是一个敷衍的回答，经过实际测试，我们发现使用不同的 id 虽然能够区分不同用户的对话，但并没有解决并发阻塞的核心问题。

所以解决问题的关键就是：**哪儿阻塞，就针对哪儿解决。**

可以为每次 AI Servi⁠ce 调用使用独立⁠生成的 ChatModel 实例。

有 2 种具体的方法：

1.  工厂模式：编写一个专门的工厂类，提供创建新 ChatModel 实例的方法
2.  Spring 多例模式：利用 Spring 的 Bean 作用域机制，从 Spring 容器中获取新的 ChatModel 实例

之前我们已经学习过工厂方案，此处选择 Spri⁠ng 多例模式实现。它更符合 ⁠Spring 的设计理念和最佳实践，而且代码实现更加简洁（不用开发新的类），维护成本也更低。

#### 开发实现

1）准备配置文件

首先，在配置文件中为不同类型的 A⁠I 模型添加专门的配置⁠参数。这样做的好处是可以根据不同的使用场景选择最适合的模型配置：

```yaml
# AI 模型配置
langchain4j:
  open-ai:
    # 推理 AI 模型配置（用于复杂的推理任务）
    reasoning-streaming-chat-model:
      base-url: https://api.deepseek.com
      api-key: <Your API Key>
      model-name: deepseek-reasoner
      max-tokens: 32768
      temperature: 0.1
      log-requests: true
      log-responses: true
    # 智能路由 AI 模型配置（用于简单的分类任务）
    routing-chat-model:
      base-url: https://api.deepseek.com
      api-key: <Your API Key>
      model-name: deepseek-chat
      log-requests: true
      log-responses: true
```

推理模型用于复杂的代码生成任务，路⁠由模型用于简单的分类⁠判断。这种分层设计既保证了功能的完整性，又为后续的成本优化打下了基础。

💡 注意这里我们为不同的模型设置了不同的参数。推理模⁠型使用更高的 max-tokens ⁠和更低的 temperature 来保证输出的稳定性；路由模型则相对简化配置，因为它主要用于简单的分类任务。

2）创建多例配置类

需要为每种模型创建对应的配置类。这里的关键是使用 `@Scope("prototype")` 注解，它告诉 Spring 容器每次获取 Bean 时都创建一个全新的实例，而不是复用单例。

在 `config` 包下编写通用流式聊天模型配置：

```java
@Configuration
@ConfigurationProperties(prefix = "langchain4j.open-ai.streaming-chat-model")
@Data
public class StreamingChatModelConfig {

    private String baseUrl;

    private String apiKey;

    private String modelName;

    private Integer maxTokens;

    private Double temperature;

    private boolean logRequests;

    private boolean logResponses;

    @Bean
    @Scope("prototype")
    public StreamingChatModel streamingChatModelPrototype() {
        return OpenAiStreamingChatModel.builder()
                .apiKey(apiKey)
                .baseUrl(baseUrl)
                .modelName(modelName)
                .maxTokens(maxTokens)
                .temperature(temperature)
                .logRequests(logRequests)
                .logResponses(logResponses)
                .build();
    }
}
```

推理专用流式模型配置：

```java
@Configuration
@ConfigurationProperties(prefix = "langchain4j.open-ai.reasoning-streaming-chat-model")
@Data
public class ReasoningStreamingChatModelConfig {

    private String baseUrl;

    private String apiKey;

    private String modelName;

    private Integer maxTokens;

    private Double temperature;

    private Boolean logRequests = false;

    private Boolean logResponses = false;

    @Bean
    @Scope("prototype")
    public StreamingChatModel reasoningStreamingChatModelPrototype() {
        return OpenAiStreamingChatModel.builder()
                .apiKey(apiKey)
                .baseUrl(baseUrl)
                .modelName(modelName)
                .maxTokens(maxTokens)
                .temperature(temperature)
                .logRequests(logRequests)
                .logResponses(logResponses)
                .build();
    }
}
```

智能路由专用模型配置：

```java
@Configuration
@ConfigurationProperties(prefix = "langchain4j.open-ai.routing-chat-model")
@Data
public class RoutingAiModelConfig {

    private String baseUrl;

    private String apiKey;

    private String modelName;

    private Integer maxTokens;

    private Double temperature;

    private Boolean logRequests = false;

    private Boolean logResponses = false;

    /**
     * 创建用于路由判断的ChatModel
     */
    @Bean
    @Scope("prototype")
    public ChatModel routingChatModelPrototype() {
        return OpenAiChatModel.builder()
                .apiKey(apiKey)
                .modelName(modelName)
                .baseUrl(baseUrl)
                .maxTokens(maxTokens)
                .temperature(temperature)
                .logRequests(logRequests)
                .logResponses(logResponses)
                .build();
    }
}
```

3）修改工厂类

更新 `AiCodeGeneratorServiceFactory` 类，让它根据代码生成类型选择不同的模型配置：

```java
// 根据代码生成类型选择不同的模型配置
return switch (codeGenType) {
    case VUE_PROJECT -> {
        // 使用多例模式的 StreamingChatModel 解决并发问题
        StreamingChatModel reasoningStreamingChatModel = SpringContextUtil.getBean("reasoningStreamingChatModelPrototype", StreamingChatModel.class);
        yield AiServices.builder(AiCodeGeneratorService.class)
                .streamingChatModel(reasoningStreamingChatModel)
                .chatMemoryProvider(memoryId -> chatMemory)
                .tools(toolManager.getAllTools())
                .hallucinatedToolNameStrategy(toolExecutionRequest -> ToolExecutionResultMessage.from(
                        toolExecutionRequest, "Error: there is no tool called " + toolExecutionRequest.name()
                ))
                .build();
    }
    case HTML, MULTI_FILE -> {
        // 使用多例模式的 StreamingChatModel 解决并发问题
        StreamingChatModel openAiStreamingChatModel = SpringContextUtil.getBean("streamingChatModelPrototype", StreamingChatModel.class);
        yield AiServices.builder(AiCodeGeneratorService.class)
                .chatModel(chatModel)
                .streamingChatModel(openAiStreamingChatModel)
                .chatMemory(chatMemory)
                .build();
    }
    default -> throw new BusinessException(ErrorCode.SYSTEM_ERROR,
            "不支持的代码生成类型: " + codeGenType.getValue());
};
```

同时，我们需要为所有原本注入 `ChatModel` 的地方指定具体的 Bean 名称，避免冲突：

```java
@Resource(name = "openAiChatModel")
private ChatModel chatModel;
```

这样修改后，每个 AI 服务实例都会获得独立的 `StreamingChatModel`，从而彻底解决并发阻塞问题。

4）更新智能路由服务

同样的思路，更新智能路由 AI 服务工厂类：

```java
@Slf4j
@Configuration
public class AiCodeGenTypeRoutingServiceFactory {

    /**
     * 创建AI代码生成类型路由服务实例
     */
    public AiCodeGenTypeRoutingService createAiCodeGenTypeRoutingService() {
        // 动态获取多例的路由 ChatModel，支持并发
        ChatModel chatModel = SpringContextUtil.getBean("routingChatModelPrototype", ChatModel.class);
        return AiServices.builder(AiCodeGenTypeRoutingService.class)
                .chatModel(chatModel)
                .build();
    }

    /**
     * 默认提供一个 Bean
     */
    @Bean
    public AiCodeGenTypeRoutingService aiCodeGenTypeRoutingService() {
        return createAiCodeGenTypeRoutingService();
    }
}
```

相应地，调用智能路由服务的地⁠方也需要调整获取⁠逻辑。比如 AppServiceImpl 中的调用：

```java
@Resource
private AiCodeGenTypeRoutingServiceFactory aiCodeGenTypeRoutingServiceFactory;

@Override
public Long createApp(AppAddRequest appAddRequest, User loginUser) {
    // 参数校验
    String initPrompt = appAddRequest.getInitPrompt();
    ThrowUtils.throwIf(StrUtil.isBlank(initPrompt), ErrorCode.PARAMS_ERROR, "初始化 prompt 不能为空");
    // 使用 AI 智能选择代码生成类型（多例模式）
    AiCodeGenTypeRoutingService routingService = aiCodeGenTypeRoutingServiceFactory.createAiCodeGenTypeRoutingService();
    CodeGenTypeEnum selectedCodeGenType = routingService.routeCodeGenType(initPrompt);
    // ... 其他业务逻辑
}
```

还有 RouterNode 中的调用：

```java
// 获取AI路由服务工厂并创建新的路由服务实例
AiCodeGenTypeRoutingServiceFactory factory = SpringContextUtil.getBean(AiCodeGenTypeRoutingServiceFactory.class);
AiCodeGenTypeRoutingService routingService = factory.createAiCodeGenTypeRoutingService();
```

#### 测试验证

编写一个专门的并发测试类，使⁠用 Java 21⁠ 的虚拟线程特性实现并发：

```java
@Slf4j
@SpringBootTest
public class AiConcurrentTest {

    @Resource
    private AiCodeGenTypeRoutingServiceFactory routingServiceFactory;

    @Test
    public void testConcurrentRoutingCalls() throws InterruptedException {
        String[] prompts = {
                "做一个简单的HTML页面",
                "做一个多页面网站项目",
                "做一个Vue管理系统"
        };
        // 使用虚拟线程并发执行
        Thread[] threads = new Thread[prompts.length];
        for (int i = 0; i < prompts.length; i++) {
            final String prompt = prompts[i];
            final int index = i + 1;
            threads[i] = Thread.ofVirtual().start(() -> {
                AiCodeGenTypeRoutingService service = routingServiceFactory.createAiCodeGenTypeRoutingService();
                var result = service.routeCodeGenType(prompt);
                log.info("线程 {}: {} -> {}", index, prompt, result.getValue());
            });
        }
        // 等待所有任务完成
        for (Thread thread : threads) {
            thread.join();
        }
    }
}
```

前面也讲过，虚拟线程是 Java 19 引入的预览特性，在 ⁠Java 21 中正式发布。与传统线程相⁠比，虚拟线程更加轻量级，可以创建数百万个实例而不会对系统造成太大负担，非常适合 I/O 密集型任务的并发测试。

运行这个测试后，可以观察到几乎所有⁠的 AI 调用都是同时⁠完成的，这说明多个线程能够并行处理 AI 请求，并发阻塞问题得到了解决。

![](https://pic.code-nav.cn/course_picture/1608440217629360130/xDmc9o0p5MlnIugf.webp)

还可以通过前端同时发起多个对话来进一步验证效果：

![](https://pic.code-nav.cn/course_picture/1608440217629360130/BBY0Bqjiztz1hXzl.webp)

### Redis 缓存优化

解决了并发问题后，我们继续优化系统的⁠响应速度。对于那些访问频⁠率高但更新频率低的数据，使用缓存可以显著减少数据库查询次数，提升用户体验。

#### 缓存策略设计

设计缓存时一般考虑几个问题：⁠存什么？怎么存？存⁠多久？怎么更新？

**一般缓存高频访问的、低频更新的数据。**

比如对于我们的项目，主页精选应用就很适合缓存。具体来说就是缓⁠存前 10 页的精选应用列表数据。因为绝⁠大多数用户只会浏览前几页的内容，很少有人会翻到第 10 页以后；而且精选应用是由管理员手动设置的，更新频率相对较低。

对于这种场景，我们采用最主流的 **旁路缓存** 模式：

-   查询时先检查缓存，命中则直接返回
-   缓存未命中则查询数据库，并将结果写入缓存
-   设置合理的过期时间，无需主动删除缓存

![](https://pic.code-nav.cn/course_picture/1608440217629360130/i5kDFnP2JUSgWlT9.webp)

而 Spring Data ⁠Redis 组件提⁠供的缓存注解，正好能够满足我们的需求，下面我们就来开发实现。

💡 除了旁路缓存模式外，常用的缓存读写策略还有读写穿透、异步缓存写入。可以看这道经典面试题来学习：[Redis 中的三种高效缓存读写策略是什么？](https://www.mianshiya.com/question/1776560599812018178#)

此外，使用缓存时，有一些注意事项，可以看这道经典面试题来学习：[Redis 中的缓存击穿、缓存穿透和缓存雪崩是什么？](https://www.mianshiya.com/question/1780933295672946690)

#### 开发实现

1）首先确保 Redis 配置正确

之前我们在引入 `spring-session-data-redis` 时已经自动引入了 `spring-data-redis`，所以不需要额外添加依赖。

检查配置文件中的 Redis 连接信息：

```yaml
spring:
  data:
    redis:
      host: localhost
      port: 6379
      database: 0
      password:
      ttl: 3600  # 缓存过期时间（秒）
```

2）缓存工具类实现

为了生成一致且唯一的缓存键，在 `utils` 包下创建一个专门的缓存工具类。缓存键的生成思路是将复杂的对象转换为固定长度的哈希值，这样既保证了不同查询请求的 key 唯一，又避免了 key 过长的问题：

```java
/**
 * 缓存 key 生成工具类
 *
 * @author yupi
 */
public class CacheKeyUtils {

    /**
     * 根据对象生成缓存key (JSON + MD5)
     *
     * @param obj 要生成key的对象
     * @return MD5哈希后的缓存key
     */
    public static String generateKey(Object obj) {
        if (obj == null) {
            return DigestUtil.md5Hex("null");
        }
        // 先转JSON，再MD5
        String jsonStr = JSONUtil.toJsonStr(obj);
        return DigestUtil.md5Hex(jsonStr);
    }
}
```

这个工具类主要使用 Hutool 工具库实现，几个要点：

1.  JSON 序列化：确保对象内容的一致性，相同内容的对象生成相同的字符串
2.  MD5 哈希：将长字符串转换为固定长度的字符串，避免 Redis key 过长
3.  边界处理：正确处理 null 值和空参数的情况

3）启用缓存功能

在 Spring Boot 启动类上添加 `@EnableCaching` 注解，支持 Spring Data 缓存注解：

```java
@EnableCaching
@SpringBootApplication
public class YuAiCodeMotherApplication {
    public static void main(String[] args) {
        SpringApplication.run(YuAiCodeMotherApplication.class, args);
    }
}
```

4）配置缓存管理器

必须配置 Redis 缓存管理器 `CacheManager`，这是 Spring Cache 的核心组件。如果不配置的话，使用缓存注解时可能会报错。

![](https://pic.code-nav.cn/course_picture/1608440217629360130/pHjD7OXftSk8kGtZ.webp)

在 `config` 包下新增代码：

```java
@Configuration
public class RedisCacheManagerConfig {

    @Resource
    private RedisConnectionFactory redisConnectionFactory;

    @Bean
    public CacheManager cacheManager() {
        // 配置 ObjectMapper 支持 Java8 时间类型
        ObjectMapper objectMapper = new ObjectMapper();
        objectMapper.registerModule(new JavaTimeModule());
        
        // 默认配置
        RedisCacheConfiguration defaultConfig = RedisCacheConfiguration.defaultCacheConfig()
                .entryTtl(Duration.ofMinutes(30)) // 默认 30 分钟过期
                .disableCachingNullValues() // 禁用 null 值缓存
                // key 使用 String 序列化器
                .serializeKeysWith(RedisSerializationContext.SerializationPair
                        .fromSerializer(new StringRedisSerializer()))
                // value 使用 JSON 序列化器（支持复杂对象）
                .serializeValuesWith(RedisSerializationContext.SerializationPair
                        .fromSerializer(new GenericJackson2JsonRedisSerializer(objectMapper)));
        
        return RedisCacheManager.builder(redisConnectionFactory)
                .cacheDefaults(defaultConfig)
                // 针对 good_app_page 配置5分钟过期
                .withCacheConfiguration("good_app_page",
                        defaultConfig.entryTtl(Duration.ofMinutes(5)))
                .build();
    }
}
```

这个配置的几个关键点：

1.  序列化器选择：`StringRedisSerializer` 用于序列化 key，确保 Redis 中的 key 是可读的字符串；`GenericJackson2JsonRedisSerializer` 用于序列化 value，支持复杂对象的序列化和反序列化。
2.  时间类型支持：注册 `JavaTimeModule` 来支持 Java 8 时间类型 LocalDateTime
3.  差异化配置：既提供了默认配置，又为特定的缓存区域设置不同的过期时间

但是要注意，如果对 value 进行 JSON 序列化，可⁠能会出现无法反序列化的情况，因为 Re⁠dis 中并没有存储 Java 类的信息，不知道要反序列化成哪个类，就会报错。所以我们可以先注释掉这些代码：

```java
// value 使用 JSON 序列化器（支持复杂对象）
// .serializeValuesWith(RedisSerializationContext.SerializationPair
//     .fromSerializer(new GenericJackson2JsonRedisSerializer(objectMapper)));
```

5）应用缓存注解

在 `AppController` 中为精选应用列表接口添加缓存注解：

```java
@PostMapping("/good/list/page/vo")
@Cacheable(
        value = "good_app_page",
        key = "T(com.yupi.yuaicodemother.utils.CacheKeyUtils).generateKey(#appQueryRequest)",
        condition = "#appQueryRequest.pageNum <= 10"
)
public BaseResponse<Page<AppVO>> listGoodAppVOByPage(@RequestBody AppQueryRequest appQueryRequest) {
    // 方法实现保持不变...
}
```

这里使用了 SpEL（Spr⁠ing Expre⁠ssion Language）表达式：

-   `T(类名)`：用于调用静态方法，生成缓存 key
-   `#参数名`：用于引用方法参数
-   `condition`：设置缓存条件，只有前 10 页才会被缓存

💡 建议了解下缓存注解的工作原理，执行流程如下：

1.  方法执行前：Spring 根据 key 表达式生成缓存键
2.  缓存检查：检查 Redis 中是否存在该键对应的缓存数据
3.  缓存命中：如果存在且未过期，直接返回缓存数据，不执行方法
4.  缓存未命中：如果不存在，执行方法获取结果，并将结果存储到 Redis 中
5.  返回结果：返回方法执行结果

#### 测试验证

通过 Redis 客户端可以⁠观察到缓存的效果。⁠每个页面的缓存都有独立的 key：

![](https://pic.code-nav.cn/course_picture/1608440217629360130/B5fdz80oZ5hxvtPc.webp)

缓存的内容以 JSON 格式存储，便于阅读和调试：

![](https://pic.code-nav.cn/course_picture/1608440217629360130/oKlU5w6ox7u8SUy4.webp)

如果没有配置序列化器，Red⁠is 中存储的会是⁠二进制数据，不便于调试：

![](https://pic.code-nav.cn/course_picture/1608440217629360130/OXPnL9Icvtr6HY1p.webp)

可以自行对比下使用缓存前后的⁠接口响应时长，看看⁠性能优化多少倍~

#### 扩展思路

1）CDN 静态资源加速：将生成的网站文件部署到 CDN ⁠服务器，通过全球节点分发减少访问延迟。⁠特别是对于热门应用，可以显著提升用户访问体验。但缺点就是会产生额外的成本，注意配置好告警，定期看看监控。

毕竟“使用 CDN，钱包两行泪”，可不是虚的。

2）缓存热门应用：统计应用的⁠访问数，对可能被频⁠繁访问的应用详情和对话列表进行缓存，提高加载速度。

[编程导航的智能面试刷题平台项目](https://www.codefather.cn/course/1826803928691945473) 第 6 期讲解过基于 HotKey 实现的热点数据自动探测和缓存能力，建议学习。

## 二、实时性优化

### 当前问题

之前有提到，如果是 Vue 工程模式生成，用户在 AI 生成完代码⁠后无法实时浏览到网站效果，或者看到的还是旧版本的⁠页面。这是因为我们之前采用的是异步打包策略，当用户看到 AI 回复完成时，Vue 项目可能还在后台构建中，存在时间差。

### 解决方案

#### 方案调研

实现实时浏览的方案较多，这里直接通过一张表格列举：



|方案|核心思路|优点|缺点|
|---|---|---|---|
|同步打包|改为同步打包|确保预览完全就绪，用户体验好|用户需要等待打包完成，而且可能会影响系统性能|
|轮询请求|前端轮询打包状态，完成后刷新|实现较为简单|请求次数可能较多，实时性一般|
|异步打包 +⁠ 进度反馈|异步打包完成后，通过 SSE（或 WebSocket）向⁠前端推送进度|异步打包后第一时间推送给前端，用户体验好|实现复杂度较高|
|预览服务热更新|监听构建文件 + WebSocket 热更新|接近实时的预览效果|实现复杂度很高|

#### 最终解决方案

显然，最简单且有效的解决方案是 **改为同步打包**。除了简单之外，还能保证逻辑更加一致。

让我们回顾一下代码生成的整体流程：

-   HTML 模式：生成的文件直接保存，用户立即可以访问
-   Vue 项目模式：生成代码后还需要安装依赖和构建，然后才能访问

从本质上讲，Vue 项目的安装⁠依赖和构建过程就是一⁠种特殊的文件保存操作，目的都是为了让用户能够立即访问生成的内容。

### 开发实现

1）移除异步构建逻辑

首先，我们需要从 `JsonMessageStreamHandler` 的 `doOnComplete` 方法中移除构建逻辑：

```java
.doOnComplete(() -> {
    // 流式响应完成后，仅添加 AI 消息到对话历史
    String aiResponse = chatHistoryStringBuilder.toString();
    chatHistoryService.addChatMessage(appId, aiResponse, ChatHistoryMessageTypeEnum.AI.getValue(), loginUser.getId());
})
```

2）在合适位置添加同步构建

为了保证跟其他模式的保存时机一致，在 `AiCodeGeneratorFacade` 的 `processTokenStream` 方法中添加构建逻辑是最合适的了：

```java
.onCompleteResponse((ChatResponse response) -> {
    // 执行 Vue 项目构建（同步执行，确保预览时项目已就绪）
    String projectPath = AppConstant.CODE_OUTPUT_ROOT_DIR + File.separator + "vue_project_" + appId;
    vueProjectBuilder.buildProject(projectPath);
    sink.complete();
})
```

注意要让外层方法额外传递 appId 参数。

这样修改后，当用户看到 AI ⁠回复完成时，Vue ⁠项目也已经构建完毕，可以立即进行预览，轻松解决了时间差问题。

### 扩展思路

1）提供构建状态查询接口。如⁠果还希望后端异步构⁠建，前端可以通过轮询来检测构建状态。

示例代码如下：

```java
@GetMapping("/build/status/{appId}")
public BaseResponse<Map<String, Object>> getBuildStatus(@PathVariable Long appId, HttpServletRequest request) {
    // 参数校验和权限检查
    ThrowUtils.throwIf(appId == null || appId <= 0, ErrorCode.PARAMS_ERROR, "应用ID无效");
    User loginUser = userService.getLoginUser(request);
    App app = appService.getById(appId);
    ThrowUtils.throwIf(app == null, ErrorCode.NOT_FOUND_ERROR, "应用不存在");
    
    if (!app.getUserId().equals(loginUser.getId()) && !UserConstant.ADMIN_ROLE.equals(loginUser.getUserRole())) {
        throw new BusinessException(ErrorCode.NO_AUTH_ERROR, "无权限查询构建状态");
    }
    // 检查构建状态
    String projectPath = AppConstant.CODE_OUTPUT_ROOT_DIR + File.separator + "vue_project_" + appId;
    File projectDir = new File(projectPath);
    File distDir = new File(projectDir, "dist");
    Map<String, Object> buildStatus = new HashMap<>();
    buildStatus.put("appId", appId);
    buildStatus.put("projectExists", projectDir.exists());
    buildStatus.put("distExists", distDir.exists());
    buildStatus.put("isBuilding", false); // 同步构建模式下总是false
    if (distDir.exists()) {
        buildStatus.put("status", "completed");
        buildStatus.put("message", "构建已完成");
        buildStatus.put("buildTime", distDir.lastModified());
    } else if (projectDir.exists()) {
        buildStatus.put("status", "pending");
        buildStatus.put("message", "项目已生成，等待构建");
    } else {
        buildStatus.put("status", "not_found");
        buildStatus.put("message", "项目不存在");
    }    
    return ResultUtils.success(buildStatus);
}
```

2）构建状态实时推送：基于 SSE 实现构建状⁠态的实时推送，替代同步打包和轮询⁠机制。当构建状态发生变化时主动推送给前端，用户体验可能会更好，并且减少了不必要的网络请求。

（如果是为了写简历，推荐使用 ⁠SSE 推送方案） ⁠                               

3）Vite Dev Server 集成：集成 Vite ⁠开发服务器，实现真正的实时预览功能。A⁠I 生成代码后立即启动开发服务器，这样用户可以实时查看到修改代码的效果，无需等待重新构建，进一步提升体验。

美团 NoCode 就是这么实现的，⁠比如在加载 iframe⁠ 中的应用时，会有一些 vite、node\_modules 相关的请求：

![](https://pic.code-nav.cn/course_picture/1608440217629360130/jQjRcEitqppe5gRm.webp)

![](https://pic.code-nav.cn/course_picture/1608440217629360130/YWkoQyZOH3eBbsJ7.webp)

包括一些热更新相关的 WebSocket 连接：

![](https://pic.code-nav.cn/course_picture/1608440217629360130/LcSZoEtAfRmYtRpK.webp)

但是集成 Vite Dev Server 的开发成本极大，需要使用 Sprin⁠g Boot 为每个 Vue 项目分配一个端口来运行⁠ Dev Server，要作为代理连接前端和 Dev Server，还要管理 Dev Server 和端口资源，不推荐使用这种方案，了解即可。

![](https://www.codefather.cn/_next/static/media/defaultCode.6873fc9a.png)

## 三、安全性优化

### 流量保护

随着平台用户的增长，我们需要实现多层⁠级的流量保护机制，防止恶⁠意攻击和资源滥用。AI 对话接口作为最核心也是成本最高的功能，更需要重点保护。

#### 实现方案 - Redisson 分布式限流

从开发成本和稳定的角度，选择⁠使用基于 Redi⁠sson 的分布式限流来实现这个功能。

[Redisson](https://github.com/redisson/redisson) 是一个在 Redis 基础上实现的 Java 驻内存数据网格。它提供了一系列的分布式 Java 常用对象，包括集合、锁、同步器、原子类、布隆过滤器、限流器等。相比于直接使用 Redis 客户端，Redisson 提供了更高层次的抽象，让开发者可以 **像使用本地对象一样使用分布式对象**。

![](https://pic.code-nav.cn/course_picture/1608440217629360130/zqFcJ1Hf23UmL8kR.webp)

对于限流功能，Redisson 实现了[基于令牌桶算法的 RRateLimiter](https://redisson.pro/docs/data-and-services/objects/#ratelimiter)。这是经典的网络流量速率限制算法，工作原理如下：

1.  令牌桶：系统维护一个固定容量的令牌桶
2.  令牌生成：以固定的速率向桶中添加令牌
3.  请求处理：每个请求需要消耗一个令牌才能被处理
4.  限流效果：当桶中没有令牌时，请求被拒绝或排队

这种算法的优势在于允许突发流⁠量（桶中有足够令牌⁠时），同时保证长期的平均速率不超过设定值。

![](https://pic.code-nav.cn/course_picture/1608440217629360130/TjvM9IvGcnNXW77F.webp)

可以看这道面试题 [了解常见的限流算法和实现原理](https://www.mianshiya.com/question/1802901573531410433)。

💡 如果想进一步学习 Redisson，可以看下 [编程导航的智能面试刷题平台项目](https://www.codefather.cn/course/1826803928691945473) 第 5 期讲的基于 Redisson 实现高效签到功能；如果想学习 Sentinel 限流，可以看下 [编程导航的智能面试刷题平台项目](https://www.codefather.cn/course/1826803928691945473) 第 7 期讲的基于 Sentinel 实现网站流量控制和熔断。

#### 开发实现

考虑到限流是一个与业务松耦合的可选功能，为了便于管理和维护，下面我们将所有限流相关的代码都放到 `ratelimit` 包中。

1）首先引入 Redisson 依赖：

```xml
<!-- Redisson -->
<dependency>
    <groupId>org.redisson</groupId>
    <artifactId>redisson</artifactId>
    <version>3.50.0</version>
</dependency>
```

在 `ratelimiter.config` 包下编写 Redisson 客户端配置，读取 Redis 相关配置并初始化 Redisson 客户端 Bean：

```java
@Configuration
public class RedissonConfig {

    @Value("${spring.data.redis.host}")
    private String redisHost;

    @Value("${spring.data.redis.port}")
    private Integer redisPort;

    @Value("${spring.data.redis.password}")
    private String redisPassword;

    @Value("${spring.data.redis.database}")
    private Integer redisDatabase;

    @Bean
    public RedissonClient redissonClient() {
        Config config = new Config();
        String address = "redis://" + redisHost + ":" + redisPort;
        SingleServerConfig singleServerConfig = config.useSingleServer()
                .setAddress(address)
                .setDatabase(redisDatabase)
                .setConnectionMinimumIdleSize(1)
                .setConnectionPoolSize(10)
                .setIdleConnectionTimeout(30000)
                .setConnectTimeout(5000)
                .setTimeout(3000)
                .setRetryAttempts(3)
                .setRetryInterval(1500);
        // 如果有密码则设置密码
        if (redisPassword != null && !redisPassword.isEmpty()) {
            singleServerConfig.setPassword(redisPassword);
        }
        return Redisson.create(config);
    }
}
```

2）新增一个 ErrorCo⁠de 自定义业务异⁠常类型，便于前端展示错误信息：

```java
TOO_MANY_REQUEST(42900, "请求过于频繁"),
```

3）创建限流类型枚举，支持接⁠口、用户、IP 多⁠个维度的限流。

在 `ratelimit.enums` 包下创建枚举类：

```java
public enum RateLimitType {
    
    /**
     * 接口级别限流
     */
    API,
    
    /**
     * 用户级别限流
     */
    USER,
    
    /**
     * IP级别限流
     */
    IP
}
```

4）为了更方便使用限流，可以⁠创建限流注解，提供⁠灵活的配置选项。

在 `ratelimit.annotation` 包下新建：

```java
@Target({ElementType.METHOD})
@Retention(RetentionPolicy.RUNTIME)
public @interface RateLimit {
    
    /**
     * 限流key前缀
     */
    String key() default "";
    
    /**
     * 每个时间窗口允许的请求数
     */
    int rate() default 10;
    
    /**
     * 时间窗口（秒）
     */
    int rateInterval() default 1;
    
    /**
     * 限流类型
     */
    RateLimitType limitType() default RateLimitType.USER;
    
    /**
     * 限流提示信息
     */
    String message() default "请求过于频繁，请稍后再试";
}
```

5）实现限流切面

在 `ratelimit.aspect` 包下创建 `RateLimitAspect` 切面类，使用 AOP 面向切面编程来实现限流逻辑。

先定义切面和注入依赖：

```java
@Aspect
@Component
@Slf4j
public class RateLimitAspect {
    @Resource
    private RedissonClient redissonClient;
    @Resource
    private UserService userService;
}
```

编写核心限流逻辑：

```java
@Before("@annotation(rateLimit)")
public void doBefore(JoinPoint point, RateLimit rateLimit) {
    String key = generateRateLimitKey(point, rateLimit);
    // 使用Redisson的分布式限流器
    RRateLimiter rateLimiter = redissonClient.getRateLimiter(key);
    rateLimiter.expire(Duration.ofHours(1)); // 1 小时后过期
    // 设置限流器参数：每个时间窗口允许的请求数和时间窗口
    rateLimiter.trySetRate(RateType.OVERALL, rateLimit.rate(), rateLimit.rateInterval(), RateIntervalUnit.SECONDS);
    // 尝试获取令牌，如果获取失败则限流
    if (!rateLimiter.tryAcquire(1)) {
        throw new BusinessException(ErrorCode.TOO_MANY_REQUEST, rateLimit.message());
    }
}
```

这是限流的核心入口方法，在标注了 `@RateLimit` 注解的方法执行前进行拦截。生成限流 key，获取 Redisson 限流器，设置限流规则。然后使用令牌桶算法进行限流判断，超限时抛出业务异常。

💡 注意，一定要为限流器设置⁠过期时间，否则 Re⁠dis 中的 key 会永不过期！长时间运行后内存占用会越来越高。

编写生成限流 key 的方法：

```java
private String generateRateLimitKey(JoinPoint point, RateLimit rateLimit) {
    StringBuilder keyBuilder = new StringBuilder();
    keyBuilder.append("rate_limit:");
    // 添加自定义前缀
    if (!rateLimit.key().isEmpty()) {
        keyBuilder.append(rateLimit.key()).append(":");
    }
    // 根据限流类型生成不同的key
    switch (rateLimit.limitType()) {
        case API:
            // 接口级别：方法名
            MethodSignature signature = (MethodSignature) point.getSignature();
            Method method = signature.getMethod();
            keyBuilder.append("api:").append(method.getDeclaringClass().getSimpleName())
                    .append(".").append(method.getName());
            break;
        case USER:
            // 用户级别：用户ID
            try {
                ServletRequestAttributes attributes = (ServletRequestAttributes) RequestContextHolder.getRequestAttributes();
                if (attributes != null) {
                    HttpServletRequest request = attributes.getRequest();
                    User loginUser = userService.getLoginUser(request);
                    keyBuilder.append("user:").append(loginUser.getId());
                } else {
                    // 无法获取请求上下文，使用IP限流
                    keyBuilder.append("ip:").append(getClientIP());
                }
            } catch (BusinessException e) {
                // 未登录用户使用IP限流
                keyBuilder.append("ip:").append(getClientIP());
            }
            break;
        case IP:
            // IP级别：客户端IP
            keyBuilder.append("ip:").append(getClientIP());
            break;
        default:
            throw new BusinessException(ErrorCode.SYSTEM_ERROR, "不支持的限流类型");
    }
    return keyBuilder.toString();
}
```

这段代码看起来复杂，但做的事情很好理解 —— 根据不⁠同的限流策略生成唯一的 Redis⁠ key，API 级别按方法名、用户级别按用户 ID、IP 级别按客户端 IP，从而支持三种限流维度。

这里还有个降级逻辑的小设计，⁠用户级别限流获取用⁠户信息失败时自动降级为 IP 限流。

还有一个工具方法，获取客户端 IP：

```java
private String getClientIP() {
    ServletRequestAttributes attributes = (ServletRequestAttributes) RequestContextHolder.getRequestAttributes();
    if (attributes == null) {
        return "unknown";
    }
    HttpServletRequest request = attributes.getRequest();
    String ip = request.getHeader("X-Forwarded-For");
    if (ip == null || ip.isEmpty() || "unknown".equalsIgnoreCase(ip)) {
        ip = request.getHeader("X-Real-IP");
    }
    if (ip == null || ip.isEmpty() || "unknown".equalsIgnoreCase(ip)) {
        ip = request.getRemoteAddr();
    }
    // 处理多级代理的情况
    if (ip != null && ip.contains(",")) {
        ip = ip.split(",")[0].trim();
    }
    return ip != null ? ip : "unknown";
}
```

6）应用限流注解

在关键的 AI 对话接口上使用限流注解：

```java
@GetMapping(value = "/chat/gen/code", produces = MediaType.TEXT_EVENT_STREAM_VALUE)
@RateLimit(limitType = RateLimitType.USER, rate = 5, rateInterval = 60, message = "AI 对话请求过于频繁，请稍后再试")
public Flux<ServerSentEvent<String>> chatToGenCode(@RequestParam Long appId,
                                                   @RequestParam String message,
                                                   HttpServletRequest request) {
    // 方法实现...
}
```

这样配置后，每个用户在 60⁠ 秒内最多只能发起⁠ 5 次 AI 对话请求，超过限制会返回友好的错误提示。

#### 测试验证

通过快速连续发起多个请求来测试限流效果：

![](https://pic.code-nav.cn/course_picture/1608440217629360130/22pqlXAPONjLsPiZ.webp)

可以在 Redis 中观察到限流器的工作状态：

![](https://pic.code-nav.cn/course_picture/1608440217629360130/vvqeQi9f80jew9cq.webp)

![](https://pic.code-nav.cn/course_picture/1608440217629360130/NRo4UoyJbld4PN4T.webp)

通过 `TTL` 命令可以查看 key 的剩余过期时间：

```shell
ttl rate_limit:user:302588523967918080
```

![](https://pic.code-nav.cn/course_picture/1608440217629360130/4x7D6wDouKrFmgqU.webp)

#### 优化 SSE 错误处理

在测试过程中，我们发现了一个问题：当限流触发时⁠，前端无法正确显示后端返回的错误⁠信息。这是因为限流异常在进入 SSE 接口之前就被抛出了，没有通过流式返回，需要特殊处理。

思路是：将限流异常消息也作为 SSE 返回给前端。

SSE 是一种服务器向客户端推送数据的技术，它基于 HTTP 协议，使用 `text/event-stream` 媒体类型。SSE 消息的标准格式是：

```plain
event: 事件类型
data: 数据内容
```

优化全局异常处理器，让它能够正确处理 SSE 请求的异常：

```java
@Hidden
@RestControllerAdvice
@Slf4j
public class GlobalExceptionHandler {

    @ExceptionHandler(BusinessException.class)
    public BaseResponse<?> businessExceptionHandler(BusinessException e) {
        log.error("BusinessException", e);
        // 尝试处理 SSE 请求
        if (handleSseError(e.getCode(), e.getMessage())) {
            return null;
        }
        // 对于普通请求，返回标准 JSON 响应
        return ResultUtils.error(e.getCode(), e.getMessage());
    }

    @ExceptionHandler(RuntimeException.class)
    public BaseResponse<?> runtimeExceptionHandler(RuntimeException e) {
        log.error("RuntimeException", e);
        // 尝试处理 SSE 请求
        if (handleSseError(ErrorCode.SYSTEM_ERROR.getCode(), "系统错误")) {
            return null;
        }
        return ResultUtils.error(ErrorCode.SYSTEM_ERROR, "系统错误");
    }

    /**
     * 处理SSE请求的错误响应
     * 
     * @param errorCode 错误码
     * @param errorMessage 错误信息
     * @return true表示是SSE请求并已处理，false表示不是SSE请求
     */
    private boolean handleSseError(int errorCode, String errorMessage) {
        ServletRequestAttributes attributes = (ServletRequestAttributes) RequestContextHolder.getRequestAttributes();
        if (attributes == null) {
            return false;
        }
        HttpServletRequest request = attributes.getRequest();
        HttpServletResponse response = attributes.getResponse();
        // 判断是否是SSE请求（通过Accept头或URL路径）
        String accept = request.getHeader("Accept");
        String uri = request.getRequestURI();
        if ((accept != null && accept.contains("text/event-stream")) || 
            uri.contains("/chat/gen/code")) {
            try {
                // 设置SSE响应头
                response.setContentType("text/event-stream");
                response.setCharacterEncoding("UTF-8");
                response.setHeader("Cache-Control", "no-cache");
                response.setHeader("Connection", "keep-alive");
                // 构造错误消息的SSE格式
                Map<String, Object> errorData = Map.of(
                    "error", true,
                    "code", errorCode,
                    "message", errorMessage
                );
                String errorJson = JSONUtil.toJsonStr(errorData);
                // 发送业务错误事件（避免与标准error事件冲突）
                String sseData = "event: business-error\ndata: " + errorJson + "\n\n";
                response.getWriter().write(sseData);
                response.getWriter().flush();
                // 发送结束事件
                response.getWriter().write("event: done\ndata: {}\n\n");
                response.getWriter().flush();
                // 表示已处理SSE请求
                return true;
            } catch (IOException ioException) {
                log.error("Failed to write SSE error response", ioException);
                // 即使写入失败，也表示这是SSE请求
                return true;
            }
        }
        return false;
    }
}
```

代码中有个小细节，我们使用自定义的 `business-error` 事件类型来区分业务异常和连接异常，避免与浏览器默认的 `error` 事件冲突。

相应地，前端 AppChat⁠Page 也需要添⁠加对自定义错误事件的处理：

```typescript
// 处理business-error事件（后端限流等错误）
eventSource.addEventListener('business-error', function (event: MessageEvent) {
  if (streamCompleted) return

  try {
    const errorData = JSON.parse(event.data)
    console.error('SSE业务错误事件:', errorData)

    // 显示具体的错误信息
    const errorMessage = errorData.message || '生成过程中出现错误'
    messages.value[aiMessageIndex].content = `❌ ${errorMessage}`
    messages.value[aiMessageIndex].loading = false
    message.error(errorMessage)

    streamCompleted = true
    isGenerating.value = false
    eventSource?.close()
  } catch (parseError) {
    console.error('解析错误事件失败:', parseError, '原始数据:', event.data)
    handleError(new Error('服务器返回错误'), aiMessageIndex)
  }
})
```

优化后的效果：

![](https://pic.code-nav.cn/course_picture/1608440217629360130/AOxBA4JhuSoPEBzm.webp)

后端返回的 SSE 格式：

![](https://pic.code-nav.cn/course_picture/1608440217629360130/uHN5ZiLZMio5YUaI.webp)

### Prompt 安全审查 - 护轨机制

除了流量保护，我们还需要防范⁠恶意输入和 Pro⁠mpt 注入攻击。

美团 NoCode 也有一些⁠防护的策略，比如在⁠将 prompt 交给 AI 执行之前，会进行一次审核：

![](https://pic.code-nav.cn/course_picture/1608440217629360130/RtS89dWhUxnfwOjL.webp)

![](https://pic.code-nav.cn/course_picture/1608440217629360130/LwGH7tBI2A0X2zU0.webp)

基于 [LangChain4j 的护轨功能](https://docs.langchain4j.dev/tutorials/guardrails)，我们可以实现很轻松地实现调用 AI 前的审查能力。

#### 什么是护轨 Guardrails？

护轨是 AI 应用中的安全机制，类⁠似于道路上的护栏，用于⁠防止恶意的 Prompt 输入、防止 AI 模型产生不当或有害的内容。

其实我们把它理解为拦截器就好了，护轨分为两种：

-   输入护轨（Input Guardrails）：在用户输入传递给 AI 模型之前进行检查和过滤
-   输出护轨（Output Guardrails）：在 AI 模型生成内容后进行检查和过滤

![](https://pic.code-nav.cn/course_picture/1608440217629360130/YAweiYL9KZe7MVMp.webp)

除了输入 Prompt 和 ⁠AI 输出结果的安⁠全校验外，你还可以利用护轨进行权限校验、日志记录等。

下面我们来利用输入护轨实现 ⁠Prompt 安全⁠审核，防止一些非法 Prompt，比如：

-   拒绝过长的 Prompt
-   拒绝包含敏感词的 Prompt
-   拒绝包含注入攻击的 Prompt

#### 开发实现

1）在 `ai.guardrail` 包下创建 `PromptSafetyInputGuardrail` 类，在用户输入传递给 AI 模型之前进行安全审查：

```java
public class PromptSafetyInputGuardrail implements InputGuardrail {

    // 敏感词列表
    private static final List<String> SENSITIVE_WORDS = Arrays.asList(
            "忽略之前的指令", "ignore previous instructions", "ignore above",
            "破解", "hack", "绕过", "bypass", "越狱", "jailbreak"
    );

    // 注入攻击模式
    private static final List<Pattern> INJECTION_PATTERNS = Arrays.asList(
            Pattern.compile("(?i)ignore\\s+(?:previous|above|all)\\s+(?:instructions?|commands?|prompts?)"),
            Pattern.compile("(?i)(?:forget|disregard)\\s+(?:everything|all)\\s+(?:above|before)"),
            Pattern.compile("(?i)(?:pretend|act|behave)\\s+(?:as|like)\\s+(?:if|you\\s+are)"),
            Pattern.compile("(?i)system\\s*:\\s*you\\s+are"),
            Pattern.compile("(?i)new\\s+(?:instructions?|commands?|prompts?)\\s*:")
    );

    @Override
    public InputGuardrailResult validate(UserMessage userMessage) {
        String input = userMessage.singleText();
        // 检查输入长度
        if (input.length() > 1000) {
            return fatal("输入内容过长，不要超过 1000 字");
        }
        // 检查是否为空
        if (input.trim().isEmpty()) {
            return fatal("输入内容不能为空");
        }
        // 检查敏感词
        String lowerInput = input.toLowerCase();
        for (String sensitiveWord : SENSITIVE_WORDS) {
            if (lowerInput.contains(sensitiveWord.toLowerCase())) {
                return fatal("输入包含不当内容，请修改后重试");
            }
        }
        // 检查注入攻击模式
        for (Pattern pattern : INJECTION_PATTERNS) {
            if (pattern.matcher(input).find()) {
                return fatal("检测到恶意输入，请求被拒绝");
            }
        }
        return success();
    }
}
```

💡 注意这里实现的只是基础检测，实际生⁠产环境中可能需要更复杂的检⁠测逻辑，包括使用 AI 模型、或者敏感词内容审核服务来检测更复杂的攻击模式。

2）集成护轨到 AI 服务

在 AI 服务工厂中集成输入护轨：

```java
yield AiServices.builder(AiCodeGeneratorService.class)
        .streamingChatModel(reasoningStreamingChatModel)
        .chatMemoryProvider(memoryId -> chatMemory)
        .tools(toolManager.getAllTools())
        .inputGuardrails(new PromptSafetyInputGuardrail())  // 添加输入护轨
        .build();
```

如果你只想给某个方法使用护轨，可以用下面这种写法：

```java
public interface Assistant {
    @InputGuardrails({ FirstInputGuardrail.class, SecondInputGuardrail.class })
    String chat(String question);
    
    String doSomethingElse(String question);
}
```

#### 测试验证

通过输入一些恶意内容来测试护轨效果：

![](https://pic.code-nav.cn/course_picture/1608440217629360130/iai7xkCGpSEOft1K.webp)

后端可以看到护轨输出的异常信⁠息：       ⁠                         

![](https://pic.code-nav.cn/course_picture/1608440217629360130/CO7vn5Lqu4IsOzgZ.webp "null")

#### 扩展思路

1）智能敏感词检测：接入阿里云内容安全服务或调⁠用专门的 AI 大模型进行敏感⁠词检测，相比静态关键词匹配，能够识别更复杂的语义攻击和隐晦表达，提升检测准确率和覆盖面。

2）动态配置管理：集成 Nacos 配置中心实⁠现敏感词库的动态维护，支持热更⁠新敏感词规则，无需重启服务。管理员可以通过配置中心实时调整检测策略，快速响应新的攻击模式。

[编程导航的智能面试刷题平台项目](https://www.codefather.cn/course/1826803928691945473) 第 7 期讲解过 Nacos 的用法和实战。

3）Prompt 重写：利用护轨机制实现 Prompt 的重写功⁠能，当检测到潜在风险内容时，自动移除或替换⁠敏感信息（顺便优化 Prompt 的专业性、或者防止超出大模型的能力），而不是直接拒绝请求，在保证安全的同时提升用户体验。

4）定制化错误反馈：优化后端⁠异常处理器逻辑，捕⁠获护轨异常并通过 SSE 向前端推送准确的错误信息。

5）审查记录：通过日志或数据库记录⁠所有拦截事件的详细信息⁠，包括触发规则、用户信息、处理结果等，便于我们持续优化系统安全性。

## 四、稳定性优化

### 重试策略 - 输出护轨

由于大模型调用存在一定不确定性，有时候可能返回不符合预期的内⁠容、或者回复中断。所以为了提升系统的稳定⁠性，我们需要让大模型调用失败时能够自动重试，并且还可以实现自定义的重试策略，在 AI 响应内容不符合要求时也自动重试。

#### LangChain4j 重试机制

其实 LangChain4j 的 ChatModel 对象本身就支持重试，可以通过配置 `max-retries` 参数修改重试次数（默认值是 2）：

```yaml
langchain4j:
  open-ai:
    chat-model:
      max-retries: 3
```

构造 ChatModel 时也可以设置重试次数：

```java
OpenAiChatModel.builder()
    .maxRetries(3)
    .build();
```

如果想自己决定重试时机和策略，可以利用 [LangChain4j 的输出护轨](https://docs.langchain4j.dev/tutorials/guardrails#output-guardrail-outcomes)，可以对 AI 的响应结果进行检测和处理，并且提供了多种结果类型。

其中最有用的几种是：

-   `success()`：允许响应通过
-   `retry()`：使用相同的输入重新调用 AI
-   `reprompt()`：添加额外的提示信息后重新调用 AI
-   `fatal()`：中断 AI 响应，抛出异常

#### 开发实现

1）在 `ai.guardrail` 包下创建 `RetryOutputGuardrail` 类来检查 AI 响应的质量，当响应不符合要求时，返回 reprompt 使用新的提示词重新生成：

```java
public class RetryOutputGuardrail implements OutputGuardrail {

    @Override
    public OutputGuardrailResult validate(AiMessage responseFromLLM) {
        String response = responseFromLLM.text();
        // 检查响应是否为空或过短
        if (response == null || response.trim().isEmpty()) {
            return reprompt("响应内容为空", "请重新生成完整的内容");
        }
        if (response.trim().length() < 10) {
            return reprompt("响应内容过短", "请提供更详细的内容");
        }
        // 检查是否包含敏感信息或不当内容
        if (containsSensitiveContent(response)) {
            return reprompt("包含敏感信息", "请重新生成内容，避免包含敏感信息");
        }
        return success();
    }
    
    /**
     * 检查是否包含敏感内容
     */
    private boolean containsSensitiveContent(String response) {
        String lowerResponse = response.toLowerCase();
        String[] sensitiveWords = {
            "密码", "password", "secret", "token", 
            "api key", "私钥", "证书", "credential"
        };
        for (String word : sensitiveWords) {
            if (lowerResponse.contains(word)) {
                return true;
            }
        }
        return false;
    }
}
```

实际生产环境，对 AI 的响应检测可以更严格些。

2）集成到 AI 服务

在 AI 服务工厂 `AiCodeGeneratorServiceFactory` 中集成输出护轨：

```java
yield AiServices.builder(AiCodeGeneratorService.class)
        .chatModel(chatModel)
        .streamingChatModel(openAiStreamingChatModel)
        .chatMemory(chatMemory)
        .inputGuardrails(new PromptSafetyInputGuardrail())
        .outputGuardrails(new RetryOutputGuardrail())
        .build();
```

💡 还可以通过护轨配置类来设置最大重试次数：

```java
OutputGuardrailsConfig outputGuardrailsConfig = OutputGuardrailsConfig.builder()
        .maxRetries(3)
        .build();
```

在构造 AI Service 时使用：

```java
return AiServices.builder(Assistant.class)
            .streamingChatModel(streamingChatModel)
            .outputGuardrails(new RetryOutputGuardrail())
            .outputGuardrailsConfig(outputGuardrailsConfig)
            .chatMemory(chatMemory)
            .build();
```

#### 测试验证

我们可以故意让 AI 返回很短的内容来测试输出护轨：

```markdown
你应该只回复 "你好" 这 2 个字
```

可以看到护轨拦截到了 AI 的响应：

![](https://pic.code-nav.cn/course_picture/1608440217629360130/9oEpc8hjbqwxbafO.webp)

从日志中可以看到，输出护轨检⁠测到响应过短，自动⁠添加了重试提示并重新发送了请求：

![](https://pic.code-nav.cn/course_picture/1608440217629360130/vnTLMZI29R5Q20Tu.webp)

#### 注意事项

经过测试，如果用了输出护轨，可能会导致流式⁠输出的响应不及时，等到 AI⁠ 输出结束才一起返回，所以如果为了追求流式输出效果，建议不要通过护轨的方式进行重试。

参考 [官方文档](https://docs.langchain4j.dev/tutorials/guardrails#output-guardrails-on-streaming-responses)：

![](https://pic.code-nav.cn/course_picture/1608440217629360130/vn7YElJgTeZzxorF.webp)

#### 扩展思路

如果想深入到某部分业务细节的重试，可以使用 [Guava Retrying 库](https://github.com/rholder/guava-retrying)，参考鱼皮的 [教程文章](https://cloud.tencent.com/developer/article/1752086) 很快就能上手。

### 工具调用优化

在复杂的 AI 应用中，AI⁠ 可能会陷入工具调⁠用的无限循环。为了防止这种情况，可以采用 2 种解决方案。

#### 方案 1 - 设置调用工具次数上限

最简单的方法是限制 AI 在⁠单次对话中连续调用⁠工具的最大次数。只需要补充 1 行代码：

```java
AiServices.builder(AiCodeGeneratorService.class)
    .maxSequentialToolsInvocations(20)  // 最多连续调用 20 次工具
    .build();
```

当达到这个限制时，LangC⁠hain4j 框架⁠会强制停止工具调用循环，防止 AI 陷入无限循环状态。

#### 方案 2 - 提供退出工具

还可以为 AI 提供一个专门的退出工具，让它能够主动结束工具调用循环。在 `ai.tools` 包下新建一个继承了 BaseTool 的工具类，这样可以被 ToolManager 自动注册：

```java
@Slf4j
@Component
public class ExitTool extends BaseTool {

    @Override
    public String getToolName() {
        return "exit";
    }

    @Override
    public String getDisplayName() {
        return "退出工具调用";
    }

    /**
     * 退出工具调用
     * 当任务完成或无需继续使用工具时调用此方法
     *
     * @return 退出确认信息
     */
    @Tool("当任务已完成或无需继续调用工具时，使用此工具退出操作，防止循环")
    public String exit() {
        log.info("AI 请求退出工具调用");
        return "不要继续调用工具，可以输出最终结果了";
    }

    @Override
    public String generateToolExecutedResult(JSONObject arguments) {
        return "\n\n[执行结束]\n\n";
    }
}
```

这种方案的优势在于给了 AI 更多的主动⁠权。AI 可以根据任⁠务完成情况主动判断是否需要继续调用工具，而不是被动地等待达到调用次数上限。

💡 退出工具的效果其实在自主实现的多步骤智能体中效果更⁠好。所以我建议如果你的应用中很⁠少出现工具调用循环的问题，可以只使用方案 1。因为每多一个工具都会增加系统的复杂性和不稳定性。

此外，还可以尝试下面的方法：

1.  调大对话记忆的容量，否则 AI 会中途断片儿，忘记已经生成了哪些文件
2.  尝试换其他的 AI 大模型
3.  优化提示词

## 五、成本优化

### AI 大模型成本控制

在实际运营中，AI 大模型的调用成⁠本是一个不可忽视的⁠因素。不同模型的定价差异很大，我们需要根据不同的使用场景选择合适的模型。

#### 成本分析

以阿里云百炼的定价为例，不同模型之间的成本差异非常显著：

![](https://pic.code-nav.cn/course_picture/1608440217629360130/LotAIkFuKsSC6css.webp)

![](https://pic.code-nav.cn/course_picture/1608440217629360130/eBHDfcR7AlFMaNo0.webp)

对于智能路由这种相对简单的分类任务，我们完全可以使用成本更低的模型。比如用阿里云百炼的 `qwen-turbo` 模型来判断应该使用哪种代码生成方案，其百万 tokens 的输出成本比 deepseek-chat 模型便宜 10 倍以上！

#### 智能路由模型配置

阿里云百炼模型也兼容 Open AI，参考 [官方文档配置](https://help.aliyun.com/zh/model-studio/use-qwen-by-calling-api)。我们可以在配置文件中为智能路由指定阿里云模型的 base-url、api-key 和低成本模型，并且限制一下 max-tokens 防止意外输出，进一步控制成本。

```yaml
langchain4j:
  open-ai:
    routing-chat-model:
      base-url: https://dashscope.aliyuncs.com/compatible-mode/v1
      api-key: <Your API Key>
      model-name: qwen-turbo
      max-tokens: 100
      log-requests: true
      log-responses: true
```

这样我们就可以在不影响代码生⁠成质量的前提下，显⁠著降低智能路由的成本，而且实测下来分类速度也快了很多~

### 扩展思路

1）AI 响应缓存：对主页示例 Prompt 的输出⁠、智能路由结果等进行缓存。可以直接⁠用字符串匹配，还可以通过相似度算法匹配用户输入，复用已有结果，大幅减少 AI 调用次数和成本。

2）用户用量统计与分析：实现详细的用量统计系统⁠，记录每个用户的 AI 调用次⁠数、Token 消耗量、生成应用数量等指标，实现精细化成本控制。（本项目后续会带大家实现）

3）存储成本优化：定期清理 COS ⁠对象存储中长期未访问的应⁠用文件和临时构建产物，或者根据访问频率和时间自动迁移文件，降低存储成本。

[编程导航的智能协同云图库项目](https://www.codefather.cn/course/1864210260732116994) 第 6 期讲解过数据沉降策略，建议学习。

4）流量成本控制：优化 AP⁠I 和 SSE 响⁠应数据结构，减少不必要的字段返回，压缩响应体积，从而减少流量。

## 最后

通过本节的系统性优化，我们的 AI 零代码应用生⁠成平台在各个方面都有了提升，大家⁠要重点学习分析问题和解决问题的思路，之后开发任何项目时，本能地想到可以从这些角度去优化系统。

这些技术和思路不仅适用于 A⁠I 应用，也可以应⁠用到其他类型的系统中，举些例子：

1.  限流机制：可以用于任何需要保护的接口，比如支付接口、短信接口等
2.  缓存策略：可以用于任何需要提升性能的场景，比如商品列表、用户信息等
3.  多例模式：可以用于任何需要避免资源竞争的组件，比如文件处理器、网络连接池等
4.  护轨机制：可以扩展到其他需要内容审核的场景，比如用户评论、文章发布等

[编程导航的智能协同云图库项目](https://www.codefather.cn/course/1864210260732116994) 第 6 期也给大家讲了很多针对图片场景的优化技巧，[鱼皮的保姆级写简历指南](https://www.codefather.cn/course/1802644557818343425/section/1802644741965066241) 中也给大家分享了 14 种优化项目的角度，强烈建议大家学习。

一定要记住，系统优化是一个持续的过程，建议大家⁠在实际项目中多实践、多思考，逐⁠步积累自己的优化经验和方法论。记住，优化是没有尽头的，没有最好的优化，只有最合适的优化！


