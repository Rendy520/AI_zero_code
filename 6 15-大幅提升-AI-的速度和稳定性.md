---
source: https://www.codefather.cn/course/1948291549923344386/section/1993925955617415169
---

# 15 - 大幅提升 AI 的速度和稳定性 -

这篇文章分享几个提高 AI ⁠生成代码的速度、稳⁠定性、质量的方法。

## 1、切换大模型

之前用的是 DeepSeek ⁠大模型，现在智谱推出⁠了更快更强的 GLM-4.6 模型，建议大家替换一下，效果会好很多！

![](https://pic.code-nav.cn/course_picture/1601072287388278786/gWguFOCovCTsSVHQ.webp)

只需要修改 `application.yml` 中的配置即可：

```yaml
langchain4j:
  open-ai:
    streaming-chat-model:
      base-url: https://open.bigmodel.cn/api/paas/v4
      api-key: <Your API Key>
      model-name: glm-4.6
      max-tokens: 8192
    reasoning-streaming-chat-model:
      base-url: https://open.bigmodel.cn/api/paas/v4
      api-key: <Your API Key>
      model-name: glm-4.6
      max-tokens: 32768
      temperature: 0.1
```

注意，如果你要使用官方提供的 [Coding Plan 的优惠套餐包](https://www.bigmodel.cn/claude-code?ic=O57MT05XKM)，需要替换 `base-url` 为 `https://open.bigmodel.cn/api/coding/paas/v4`（中间多加了个 coding）。

如果你要生成更复杂的项目，max-tokens 可以调⁠大点，GLM-4.6 有 200K ⁠超长上下文窗口，写几十个页面都不成问题。                                

这里鱼皮是强烈推荐大家 **个人做项目时** 使用智谱的 [Coding Plan 套餐包](https://www.bigmodel.cn/claude-code?ic=O57MT05XKM) 的（点击链接可以优惠购买），直接包月，不按 tokens 数计费。但是如果是对外提供商业服务，还是更推荐直接使用 tokens，否则并发数量可能无法满足需求。

![](https://pic.code-nav.cn/course_picture/1601072287388278786/AdzS3YwCfc2n5S4q.webp)

## 2、调大上下文窗口

如果 AI 有时无法正常完成任务、一直循环，建议调大 AiCode⁠GeneratorServiceFactor⁠y 的上下文窗口 maxMessages，防止 AI 断片儿：                                

```java
MessageWindowChatMemory chatMemory = MessageWindowChatMemory
        .builder()
        .id(appId)
        .chatMemoryStore(redisChatMemoryStore)
        .maxMessages(50)
        .build();
```

## 3、设置超时时间

如果发现 AI 执行到一半，⁠突然报错 "rea⁠d timeout"，那可能是生成的代码太长，导致超时了。

可以修改配置类 ReasoningStreamingChatMo⁠delConfig 和 Streaming⁠ChatModelConfig，补充超时时间配置：                                

```java
public class ReasoningStreamingChatModelConfig {

    // ... 省略前面的字段

    // 新增超时时间
    private Duration timeout;

    /**
     * 推理流式模型（用于 Vue 项目生成，带工具调用）
     */
    @Bean
    @Scope("prototype")
    public StreamingChatModel reasoningStreamingChatModelPrototype() {
        return OpenAiStreamingChatModel.builder()
                .apiKey(apiKey)
                .baseUrl(baseUrl)
                .modelName(modelName)
                .maxTokens(maxTokens)
                .temperature(temperature)
                .timeout(timeout) // 新增超时时间配置
                .logRequests(logRequests)
                .logResponses(logResponses)
                .build();
    }
}
```

然后在 `application.yml` 配置中添加超时时间配置即可：

```yaml
langchain4j:
  open-ai:
    streaming-chat-model:
      base-url: https://open.bigmodel.cn/api/coding/paas/v4
      api-key: <Your API Key>
      model-name: glm-4.6
      max-tokens: 8192
      timeout: 300s
   reasoning-streaming-chat-model:
      base-url: https://open.bigmodel.cn/api/coding/paas/v4
      api-key: <Your API Key>
      model-name: glm-4.6
      max-tokens: 32768
      temperature: 0.1
      timeout: 300s
```


